{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penalty Functions: Example of How $L_p$ Penalty Changes Loss Optima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "from ipywidgets import interact,interact_manual, FloatSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate noisy data from a line\n",
    "\n",
    "######################\n",
    "# Change Me!\n",
    "a_true = 5\n",
    "b_true = 2\n",
    "noise = 0.001\n",
    "#################\n",
    "\n",
    "true_func = lambda x: a_true*x+b_true\n",
    "\n",
    "num_samples = 50\n",
    "x = (np.random.rand(num_samples)-0.5)*20\n",
    "y = true_func(x)+np.random.normal(scale=noise,size=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "def loss(a,b,alpha,order=2):\n",
    "    #return np.linalg.norm(y - (a*x+b),ord=2)\n",
    "    #return np.average((y - (a*x+b))**2) + alpha*(a**2) + alpha*(b**2)\n",
    "    return np.average((y - (a*x+b))**2) + alpha*norm([a,b],ord=order)\n",
    "\n",
    "#example\n",
    "loss(a=5,b=2,alpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = np.meshgrid(np.linspace(-10, 10, 201), np.linspace(-10, 10, 201))\n",
    "N,M = A.shape\n",
    "floss = np.vectorize(loss)\n",
    "\n",
    "def generate_new_data(a=5,b=5):\n",
    "    x = (np.random.rand(num_samples)-0.5)*20\n",
    "    #y = true_func(x)+np.random.normal(scale=1,size=num_samples)\n",
    "    y = a*x+b+np.random.normal(scale=1,size=num_samples)\n",
    "    return x,y\n",
    "\n",
    "def generate_z_grid(alpha,order):\n",
    "    Z_noalpha = floss(A.flatten(),B.flatten(),0).reshape((N,M))\n",
    "    alpha=alpha\n",
    "    Z = floss(A.flatten(),B.flatten(),alpha,order)\n",
    "    min_ind = np.argmin(Z)\n",
    "    Amin = A.flatten()[min_ind]\n",
    "    Bmin = B.flatten()[min_ind]\n",
    "    Z = Z.reshape((N,M))\n",
    "    return Z_noalpha, Z, Amin, Bmin\n",
    "\n",
    "get_levels = lambda z: [np.percentile(z.flatten(),i) for i in [0,0.5,1,2,5,10,15,30,40,50,75,95,100]]\n",
    "#levels = [np.percentile(allz,i) for i in [0,0.5,1,2,5,10,15,30,40,50,75,95,100]]\n",
    "#levels = [np.percentile(allz,i) for i in np.logspace(-2,3,10,base=3)]\n",
    "\n",
    "def plot_objective(alpha=0,order=2):\n",
    "    Z_noalpha, Z, Amin, Bmin = generate_z_grid(alpha,order)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.vlines(0,-10,10,alpha=0.25,colors='r',linestyles='solid')\n",
    "    plt.hlines(0,-10,10,alpha=0.25,colors='r',linestyles='solid')\n",
    "    plt.contour(A,B,Z_noalpha,10,levels=get_levels(Z_noalpha),linestyles='solid',cmap='Greys_r',alpha=0.05)\n",
    "    plt.contour(A,B,Z,10,levels=get_levels(Z),linestyles='solid',cmap='Greys_r',alpha=0.5)\n",
    "    plt.scatter([0],[0],marker='D',s=50,c='r')\n",
    "    plt.scatter([a_true],[b_true],marker='*',s=400)\n",
    "    plt.scatter([Amin],[Bmin])\n",
    "    plt.xlabel('a')\n",
    "    plt.ylabel('b')\n",
    "    plt.title('Optima: a={:.2f}, b={:.2f}'.format(Amin,Bmin))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_objective,\n",
    "         alpha=np.logspace(-2,5,8),\n",
    "         order=FloatSlider(min=0,max=10,step=0.1,continuous_update=False,value=2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How about plotting the path?\n",
    "For this, let's try plotting, for different norm orders, the path that the coefficients take as we set alpha = 0 (no regularization) to a large number (essentially fully regularized):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', RuntimeWarning)\n",
    "\n",
    "alpha_range = np.logspace(-1,5,14)\n",
    "order_range = [0,0.25,.5,.75,1,1.5,2,3,5,10,20,100]\n",
    "\n",
    "Al = len(alpha_range)\n",
    "Ol = len(order_range)\n",
    "results = np.zeros((Al,Ol,2))\n",
    "\n",
    "for j,o in enumerate(order_range):\n",
    "    prev_opt = [5,5]\n",
    "    for i,a in enumerate(alpha_range):\n",
    "        f = lambda x: loss(x[0],x[1],alpha=a,order=o)\n",
    "        x_opt = fmin(f,prev_opt,disp=False)\n",
    "        results[i,j] = x_opt\n",
    "        prev_opt = x_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,o in enumerate(order_range):\n",
    "    fig = plt.figure(figsize=(12,7))\n",
    "    Z_noalpha, Z, Amin, Bmin = generate_z_grid(10000,o)\n",
    "    plt.contour(A,B,Z_noalpha,10,levels=get_levels(Z_noalpha),linestyles='solid',alpha=0.2,colors='g')\n",
    "    plt.contour(A,B,Z,10,levels=get_levels(Z),linestyles='solid',alpha=0.2,colors='k')\n",
    "    plt.plot(results[:,j,0],results[:,j,1],marker='o',alpha=0.75)\n",
    "    plt.title('L-{} Norm'.format(o))\n",
    "    plt.axis('equal')\n",
    "    plt.xlim([-1,6])\n",
    "    plt.ylim([-1,3])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of How to Select a Loss or Penalty function\n",
    "In class, we reviewed a couple of different forms of loss functions and penalty functions and talked a bit about the criteria for selecting them. Below is a very short summary of these.\n",
    "\n",
    "## Loss Functions for Regression:\n",
    "* If you have no prior knowledge of the function or data, then selecting an L2 type loss (like the Mean Squared Error) is reasonable. When data nicely behaves w.r.t. a linear model (e.g., features are uncorrelated, errors in the linear model are uncorrelated, have equal variances, and expected error of zero around the linear model, etc.) then a Linear Model with an L2 Loss is the Best Linear Unbiased Estimate (BLUE) according to the [Gauss-Markov Theorem](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem).\n",
    "* If you have reason to believe that the data will have outliers or otherwise need to be robust to spurious large samples, then L2 loss will not be robust to this (as we saw in Lecture). For this, moving to an L1 type loss (like an Absolute Loss or Huber loss) will make the model less sensitive to outliers. It is one approach to handling [Robust Regression](https://en.wikipedia.org/wiki/Robust_regression).\n",
    "* If you need to have the model's loss be dominated only by a handful of points/data as opposed to all of the data, then epsilon-insensitive loss is appropriate since many points well-fit by the model will have \"zero\" loss. For things like Linear Models, this has limited usefulness right now. However, when we \"kernalize\" Linear models in \"Kernels\" week, you will see that this is a big deal, and it is what gives rise to the \"Support Vector\" part of \"Support Vector Machines\". Specifically, decreasing epsilon towards zero increases the number of needed Support Vectors (can be a bad thing), and increasing epsilon can decrease the number of needed Support Vectors (can be a good thing). This will make more sense in a few week's time.\n",
    "\n",
    "## Loss Functions for Classification:\n",
    "* Zero-One loss sounds nice, but is not useful in practice, since it is not differentiable.\n",
    "* [Perceptron loss](https://en.wikipedia.org/wiki/Perceptron), while of historical importance, is not terribly useful in practice, since it does not converge to a unique solution and an SVM (i.e., Hinge Loss below) has all of the same benefits.\n",
    "* If you need a simple linear model which outputs actual probabilities (like, 95% sure the component has failed), then the log-loss does this, via [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) and allows you to calculate classification probabilities in closed form.\n",
    "* If you want something that [maximizes the margin](https://en.wikipedia.org/wiki/Margin_(machine_learning)) of the classifier, then the Hinge Loss can get close to this. It is the basis of [Linear Support Vector Machines](https://en.wikipedia.org/wiki/Support_vector_machine#Linear_SVM)\n",
    "\n",
    "## Penalty Terms (Lp Norms) for Linear Models:\n",
    "* $L_2$ Norm penalties on the weight vector essentially \"shrink\" the weights towards zero as you increase the penalty weight. Adding this kind of penalty to a linear model has different names, depending on which community of people you are talking with. Some of these other names are: (1) [Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization), (2) Ridge Regression, (3) $L_2$ Shrinkage Estimators, or (4) Gaussian Weight Priors. I find looking at the penalty term itself more helpful at understanding the effects rather than memorizing the different names.\n",
    "* $L_0$ Norm penalties, while conceptually interesting since they essentially \"count\" entries in the weight vector, are not practically useful since they are not differentiable and are thus difficult to optimize.\n",
    "* $L_1$ Norm penalties are a compromise between $L_2$ and $L_0$ in that they promote sparsity in the weights (some weights will become zero) but are (largely) differentiable, meaning that you can meaningfully optimize them (unlike $L_0$). Shrinking certain weights to zero in this way can be useful when (1) you are in the $n \\ll p$ regime (many more features than data points) where the model is underdetermined and (2) you want some degree of model interpretability (it sets many weights to zero). Some of these other names for this kind of linear regression with this penalty are the [LASSO (least absolute shrinkage and selection operator)](https://en.wikipedia.org/wiki/Lasso_(statistics)).Â \n",
    "* $L_\\infty$ (where p is really large) essentially penalize the size of the biggest element of the weight vector (w). While there are some niche instances where this kind of norm is useful for a Loss function (e.g., [Chebyshev Regression](https://en.wikipedia.org/wiki/Minimax_approximation_algorithm)), I have rarely seen meaningful use cases in practice where this makes sense as a penalty term for the weight vector.\n",
    "* Combinations of penalties. For example, a common combination is combining both an $L_2$ and $L_1$ penalty on the weights, as in: $\\Omega(w) = \\alpha ||w||_2 + \\beta ||w||_1$. This particular combination is called [\"Elastic Net\"](https://en.wikipedia.org/wiki/Elastic_net_regularization) and exhibits some of the good properties of $L_2$ penalities with the sparsity inducing properties of $L_1$ regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
