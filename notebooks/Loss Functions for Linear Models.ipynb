{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "def modified_huber_loss(y):\n",
    "    if(abs(y)<1):\n",
    "        return y**2\n",
    "    else:\n",
    "        return 2*abs(y)-1\n",
    "mhuber = np.vectorize(modified_huber_loss)\n",
    "\n",
    "eps = 0.7\n",
    "def sq_esp_insensitive(y):\n",
    "    if(abs(y)<eps):\n",
    "        return 0\n",
    "    else:\n",
    "        return (abs(y)-eps)**2\n",
    "sq_eps_ins = np.vectorize(sq_esp_insensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax = -4, 4\n",
    "xx = np.linspace(xmin, xmax, 100)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(xx, xx**2, 'g-',\n",
    "         label=\"Squared Loss\")\n",
    "plt.plot(xx, abs(xx), 'g--',\n",
    "         label=\"Absolute Loss\")\n",
    "\n",
    "plt.plot(xx, abs(xx)-eps, 'b--',\n",
    "         label=\"Epsilon-Insensitive Loss\")\n",
    "\n",
    "plt.plot(xx, sq_eps_ins(xx), 'b-',\n",
    "         label=\"Sq-Epsilon-Insensitive Loss\")\n",
    "plt.plot(xx, mhuber(xx), 'r-',\n",
    "         label=\"Modified-Huber Loss\")\n",
    "\n",
    "plt.ylim((0, 8))\n",
    "plt.legend(loc=\"upper center\")\n",
    "plt.xlabel(\"Error$\")\n",
    "plt.ylabel(\"$L(y, f(x))$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out Outliers\n",
    "from sklearn.datasets import make_regression\n",
    "n_samples = 1000\n",
    "n_outliers = 50\n",
    "\n",
    "Xr, yr, coef = make_regression(n_samples=n_samples, n_features=1,\n",
    "                              n_informative=1, noise=10,\n",
    "                              coef=True, random_state=0)\n",
    "# Add outlier data\n",
    "np.random.seed(0)\n",
    "Xr[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1))\n",
    "yr[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers)\n",
    "yr/=10\n",
    "yr += 10\n",
    "\n",
    "line_X = np.arange(-5, 5)\n",
    "figure = plt.figure(figsize=(9, 9))\n",
    "\n",
    "plt.scatter(Xr, yr,facecolors='None',edgecolors='k',alpha=0.5)\n",
    "\n",
    "# Loss Options: huber, squared_error, epsilon_insensitive, squared_epsilon_insensitive\n",
    "losses = ['huber', 'squared_error']\n",
    "for loss in losses:\n",
    "    model = SGDRegressor(loss=loss, fit_intercept=True, max_iter = 2000,\n",
    "                     penalty='l2', alpha=.001, epsilon=1, tol=1e-3)\n",
    "    model.fit(Xr, yr)\n",
    "\n",
    "    # Predict data of estimated models\n",
    "    line_y = model.predict(line_X[:, np.newaxis])\n",
    "    plt.plot(line_X, line_y, '-', label=loss,alpha=1)\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_features=1, n_redundant=0, \n",
    "                           n_informative=1,\n",
    "                           random_state=1,\n",
    "                           n_clusters_per_class=1,\n",
    "                           flip_y=0.0, class_sep=1)\n",
    "plt.figure()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('True or False')\n",
    "plt.scatter(X,y,marker='o',facecolors='none',edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDRegressor(loss='squared_error', fit_intercept=True, max_iter = 2000,\n",
    "                     penalty='l2', alpha=.001, epsilon=1, tol=1e-3)\n",
    "model.fit(X, y)\n",
    "Xp = np.linspace(X.min(),X.max(),100)\n",
    "Xp = Xp[:, np.newaxis]\n",
    "plt.figure()\n",
    "plt.scatter(X,y,marker='o',facecolors='none',edgecolors='k')\n",
    "plt.plot(Xp,model.predict(Xp))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Loss Functions\n",
    "\n",
    "For many linear classification problems, we can use a decision function:\n",
    "$$\n",
    "y_i\\cdot(w\\cdot x_i)\n",
    "$$\n",
    "where $y_i = \\pm 1$ such that if $y_i$ and $w\\cdot x_i$ point have similar signs, then the decision function is positive, otherwise it is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html\n",
    "def modified_huber_loss(y_true, y_pred):\n",
    "    z = y_pred * y_true\n",
    "    loss = -4 * z\n",
    "    loss[z >= -1] = (1 - z[z >= -1]) ** 2\n",
    "    loss[z >= 1.] = 0\n",
    "    return loss\n",
    "\n",
    "\n",
    "xmin, xmax = -4, 4\n",
    "xx = np.linspace(xmin, xmax, 100)\n",
    "lw = 2\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.plot([xmin, 0, 0, xmax], [1, 1, 0, 0], color='gold', lw=lw,\n",
    "         label=\"Zero-one loss\")\n",
    "plt.plot(xx, -np.minimum(xx, 0), color='yellowgreen', lw=lw,\n",
    "         label=\"Perceptron loss\")\n",
    "plt.plot(xx, modified_huber_loss(xx, 1), color='darkorchid', lw=lw,\n",
    "         linestyle='--', label=\"Modified Huber loss\")\n",
    "plt.plot(xx, np.log2(1 + np.exp(-xx)), color='cornflowerblue', lw=lw,\n",
    "         label=\"Log loss\")\n",
    "plt.plot(xx, np.where(xx < 1, 1 - xx, 0), color='teal', lw=lw,\n",
    "         label=\"Hinge loss\")\n",
    "plt.plot(xx, np.where(xx < 1, 1 - xx, 0) ** 2, color='orange', lw=lw,\n",
    "         label=\"Squared hinge loss\")\n",
    "plt.ylim((0, 8))\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(r\"Decision function $f(x)$\")\n",
    "plt.ylabel(\"$L(y, f(x))$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "#loss = 'squared_error'\n",
    "loss = 'log_loss'\n",
    "#loss = 'hinge'\n",
    "\n",
    "model = SGDClassifier(loss=loss, fit_intercept=True, max_iter = 2000,\n",
    "                     penalty='l2', alpha=.001, epsilon=1, tol=1e-3)\n",
    "model.fit(X, y)\n",
    "Xp = np.linspace(X.min(),X.max(),100)\n",
    "Xp = Xp[:, np.newaxis]\n",
    "plt.figure()\n",
    "plt.scatter(X,y,marker='o',facecolors='none',edgecolors='k')\n",
    "try:\n",
    "    plt.plot(Xp,model.predict_proba(Xp)[:,1],label='probability')\n",
    "except:\n",
    "    pass\n",
    "plt.plot(Xp,model.predict(Xp))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, \n",
    "                           n_informative=2,\n",
    "                           random_state=1,\n",
    "                           n_clusters_per_class=1,\n",
    "                           flip_y=0.0, class_sep=0.7)\n",
    "#rng = np.random.RandomState(2)\n",
    "#X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "# Change: try 0,1, or 2\n",
    "ds = datasets[2]\n",
    "\n",
    "X, y = ds\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "plt.scatter(X[y==1,0],X[y==1,1],marker='+')\n",
    "plt.scatter(X[y==0,0],X[y==0,1],marker='o',facecolors='none',edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Try modifying these:\n",
    "#====================\n",
    "loss = 'squared_error'\n",
    "#loss = 'perceptron'\n",
    "#loss = 'log_loss'\n",
    "#loss = 'hinge'\n",
    "#loss = 'modified_huber'\n",
    "#loss = 'squared_hinge'\n",
    "\n",
    "# Also try the effect of Alpha:\n",
    "# e.g., between ranges 1e-20 and 1e0\n",
    "#=============================\n",
    "alpha=1e-3\n",
    "\n",
    "# You can also try other models by commenting out the below:\n",
    "model = SGDClassifier(loss=loss, fit_intercept=True,\n",
    "                      max_iter=200,tol=1e-5, n_iter_no_change =100,\n",
    "                      penalty='l2',alpha=alpha) \n",
    "#model = SGDClassifier(loss = 'hinge')\n",
    "#model = LinearSVC(loss='hinge',C=1e3)\n",
    "model.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "\n",
    "h=0.01\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "if hasattr(model, \"decision_function\"):\n",
    "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "else:\n",
    "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "vmax = max(abs(Z.min()),abs(Z.max()))\n",
    "cm = plt.cm.RdBu\n",
    "plt.contourf(xx, yy, Z, cmap=cm, alpha=.5, vmax = vmax, vmin = -vmax)\n",
    "levels = [-1.0, 0.0, 1.0]\n",
    "linestyles = ['dashed', 'solid', 'dashed']\n",
    "colors = 'k'\n",
    "plt.contour(xx, yy, Z, levels, colors=colors, linestyles=linestyles)\n",
    "plt.scatter(X[y==1,0],X[y==1,1],marker='+')\n",
    "plt.scatter(X[y==0,0],X[y==0,1],marker='o',facecolors='none',edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
