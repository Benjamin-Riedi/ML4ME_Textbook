{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Evaluating Machine Learning Models\n",
    "author: Mark Fuge\n",
    "date: 'September 19 2025'\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the prior chapter, we covered how different loss functions and regularization terms affected a linear model, in terms of the model's qualitative performance and its affect on the training score. However, as we saw, the training score an be a misleading performance indicator. How might we judge the model's performance more rigorously? This chapter addresses this by reviewing the why and how of performing Cross Validation, and also what this means regarding optimizing the hyperparameters of a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "pal = sns.color_palette(\"Paired\")\n",
    "cmap = sns.blend_palette(pal,as_cmap=True)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Number of data points\n",
    "n_samples = 30\n",
    "\n",
    "# True Function we want to estimate\n",
    "true_fun = lambda X: np.cos(1.5 * np.pi * X)\n",
    "\n",
    "# Noisy Samples from the true function\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "# Plot the true function:\n",
    "X_plot = np.linspace(0, 1, 100)\n",
    "plt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n",
    "# Plot the data samples\n",
    "plt.scatter(X,y, label=\"Samples\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting an Unbiased Estimate of Out-of-Sample Performance\n",
    "\n",
    "When we train a machine learning model, we are typically interested in how well the model will perform on data it has not seen before. This is often referred to as the model's generalization performance. However, if we evaluate the model's performance on the same data it was trained on, we may get an overly optimistic estimate of its true performance. This is because the model may have simply memorized the training data, rather than learning the underlying patterns. One popular way to assess this is through [Cross Validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "# Now let's split the data into training and test data:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the above has actually done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train\\n',X_train,'\\n')\n",
    "print('X_test\\n',X_test,'\\n')\n",
    "print('y_train\\n',y_train,'\\n')\n",
    "print('y_test\\n',y_test,'\\n')\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n",
    "# plot the training and testing points in colors\n",
    "plt.scatter(X_train,y_train, label=\"Training data\")\n",
    "plt.scatter(X_test,y_test, label=\"Testing data\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea in cross validation is to test the model on data that was separate from the data you trained on, therefore establishing two datasets: a training set and a test set. The training set is used to fit the model, while the test set is used to evaluate its performance. This way, we can get a more realistic estimate of how well the model will perform on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0, 1e-20, 1e-10, 1e-7, 1e-5, 1,10]\n",
    "d=15\n",
    "for a in alphas:\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    #plt.setp(ax, xticks=(), yticks=())\n",
    "    polynomial_features = PolynomialFeatures(degree=d,\n",
    "                                             include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    linear_regression = Ridge(alpha=a)\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    #pipeline.fit(X[:, np.newaxis], y)\n",
    "    pipeline.fit(X_train[:, np.newaxis], y_train)\n",
    "    # Evaluate the models using crossvalidation\n",
    "    #scores = cross_validation.cross_val_score(pipeline,\n",
    "    #    X[:, np.newaxis], y, scoring=\"mean_squared_error\", cv=10)\n",
    "    \n",
    "    testing_score = pipeline.score(X_test[:, np.newaxis],y_test)\n",
    "    training_score = pipeline.score(X_train[:, np.newaxis],y_train)\n",
    "\n",
    "    X_plot = np.linspace(0, 1, 100)\n",
    "    plt.plot(X_plot, pipeline.predict(X_plot[:, np.newaxis]), label=\"Model\")\n",
    "    plt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n",
    "    plt.scatter(X, y, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    plt.title(\"Degree {}, Alpha {}\\nTest score = {:.3f}\\nTraining score = {:.3f}\".format(\n",
    "        d, a, testing_score,training_score))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simplified type of cross-validation often referred to as \"Shuffle Splitting\" and is one of the most common, but it is useful to review other types of cross-validation via [this nice summary page from the SKLearn library](https://scikit-learn.org/stable/modules/cross_validation.html), which covers a variety of important variants including:\n",
    "\n",
    "1. K-Fold Cross Validation\n",
    "2. Leave-One-Out Cross Validation\n",
    "3. Stratified Cross Validation\n",
    "4. Group-wise Cross Validation\n",
    "5. Time Series Split Cross Validation\n",
    "\n",
    "**Discussion Point**: Under what conditions or situations would using each type of cross-validation above be appropriate versus inappropriate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Optimal Hyper-parameters\n",
    "\n",
    "Now that we have introduced the usage of hyper-parameters and cross-validation, a natural question arises: How do we choose the hyper-parameters? There are many ways to do this, and this section will describe the most common and basic ones, while leaving more advanced techniques (like Implicit Differentiation) for later. Specifically, this section will:\n",
    "1. Define the concepts of Grid and Random Hyper-parameter search.\n",
    "2. Use Grid and Random search to optimize hyper-parameters of a model.\n",
    "2. Distinguish when Randomized Search is much better than grid search.\n",
    "3. Describe how Global Optimization procedures such as Bayesian Optimization work.\n",
    "4. Recognize why none of those at all work in High Dimensions and describe the \"Curse of Dimensionality\"\n",
    "\n",
    "In future chapters once we cover more advanced derivative methods, we can discuss how to use tools like Implicit Differentiation to directly compute the gradient of the cross-validation score with respect to hyper-parameters, and then use this gradient to optimize the hyper-parameters using standard gradient-based optimization methods. However, for now, let's focus on more basic derivative-free methods, since they are more widely used and easier to understand.\n",
    "\n",
    "Let's start by returning to our Polynomial example, and this time focus on finding the best combination of degree and penalty weight for a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "# Let's plot the behavior of a fixed degree polynomial\n",
    "degree = 15\n",
    "# (i.e., f(x) = w_1*x + w_2*x^2 + ... + w_15*x^15)\n",
    "# but where we change alpha.\n",
    "alphas = np.logspace(start=-13,stop=4,num=20)\n",
    "polynomial_features = PolynomialFeatures(degree=degree,\n",
    "                                         include_bias=False)\n",
    "scores = []\n",
    "for a in alphas:\n",
    "    linear_regression = Ridge(alpha=a)\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    cv_scores = model_selection.cross_val_score(pipeline,\n",
    "        X[:,np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=20)\n",
    "    scores.append(cv_scores)\n",
    "\n",
    "scores = np.array(scores)\n",
    "\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.semilogx(alphas,-np.mean(scores,axis=1),'-')\n",
    "plt.ylabel('Test MSE')\n",
    "plt.xlabel('Alpha ($\\\\alpha$)')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we have more than one variable?\n",
    "Let's look at both polynomial degree and regularization weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "alphas = np.logspace(start=-13, # Start at 1e-13\n",
    "                     stop=4,    # Stop at 1e4\n",
    "                     num=40)    # Split that into 40 pieces\n",
    "degrees = range(1,16) # This will only go to 15, due to how range works\n",
    "\n",
    "scores = np.zeros(shape=(len(degrees), # i.e., 15\n",
    "                         len(alphas))) # i.e., 20\n",
    "\n",
    "for i, degree in enumerate(degrees): # For each degree\n",
    "    polynomial_features = PolynomialFeatures(degree=degree,\n",
    "                                             include_bias=False)\n",
    "    \n",
    "    for j,a in enumerate(alphas):    # For each alpha\n",
    "        linear_regression = Ridge(alpha=a)\n",
    "        pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                             (\"linear_regression\", linear_regression)])\n",
    "        cv_scores = model_selection.cross_val_score(pipeline,\n",
    "            X[:,np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=20)\n",
    "        scores[i][j] = -np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "Xs, Ys = np.meshgrid(range(len(degrees)), range(len(alphas)))\n",
    "zs = np.array([scores[i,j] for i,j in zip(np.ravel(Xs), np.ravel(Ys))])\n",
    "Zs = zs.reshape(Xs.shape)\n",
    "\n",
    "Xs, Ys = np.meshgrid(degrees, np.log(alphas))\n",
    "\n",
    "ax.plot_surface(Xs, Ys, Zs, rstride=1, cstride=1, cmap=cm.coolwarm,\n",
    "    linewidth=0, antialiased=False)\n",
    "\n",
    "# Label the Axes\n",
    "ax.set_xlabel('Degree')\n",
    "ax.set_ylabel('Regularization')\n",
    "ax.set_zlabel('MSE')\n",
    "\n",
    "# Rotate the image\n",
    "ax.view_init(30, # larger # goes \"higher\"\n",
    "             30) # larger # \"circles around\"\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,10))\n",
    "plt.imshow(Zs,\n",
    "           cmap=cm.coolwarm, # Allows you to set the color\n",
    "           vmin=Zs.min(), vmax=0.2, # The min and max Z-Values (for coloring purposes)\n",
    "           extent=[Xs.min(), Xs.max(),   # How far on X-Axis you want to plot\n",
    "                   Ys.min(), Ys.max()],  # How far on Y-Axis\n",
    "           interpolation='spline16',      # How do you want to interpolate values between data?\n",
    "           origin='lower')\n",
    "plt.title('Mean Squared Error')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Regularization')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "At the end of the day, all this is doing is optimization/search over different parameters.\n",
    "\n",
    "How should we go about automating this?\n",
    "\n",
    "Most common: Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('parameters we could change:')\n",
    "for k in pipeline.get_params().keys():\n",
    "    print(\" \",k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'polynomial_features__degree': list(range(1,16)), # 15 possible\n",
    "              'linear_regression__alpha': np.logspace(start=-13,stop=4,num=10),\n",
    "              'polynomial_features__include_bias':[True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we want to do cross-validation?\n",
    "from sklearn import model_selection\n",
    "num_data_points = len(y)\n",
    "\n",
    "# 4-fold CV\n",
    "kfold_cv = model_selection.KFold(n_splits = 4) \n",
    "\n",
    "# Or maybe you want randomized splits?\n",
    "shuffle_cv = model_selection.ShuffleSplit(n_splits = 20,     # How many iterations?\n",
    "                                          test_size=0.2    # What % should we keep for test?\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X[:,np.newaxis]\n",
    "grid_search = GridSearchCV(pipeline,    # The thing we want to optimize\n",
    "                           parameters,  # The parameters we will change\n",
    "                           cv=shuffle_cv, # How do you want to cross-validate?\n",
    "                           scoring = 'neg_mean_squared_error'\n",
    "                          )\n",
    "grid_search.fit(X, y) # This runs the cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_ # Once finished, you can see what the best parameters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best MSE for Grid Search: {:.2e}\".format(-grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.predict(X)  # You can also use the best model directly (in sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_degree = grid_search.best_params_['polynomial_features__degree']\n",
    "best_alpha = grid_search.best_params_['linear_regression__alpha']\n",
    "X_plot = X_plot[:,np.newaxis]\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(X_plot, grid_search.predict(X_plot),'-',label=\"Model\",alpha=0.5)\n",
    "plt.plot(X_plot, true_fun(X_plot), ':',label=\"True function\",alpha=1)\n",
    "plt.scatter(X,y, c='Blue', s=20, edgecolors='none')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "sns.despine()\n",
    "plt.title(\"Degree {}, Alpha {:.1e}\".format(best_degree,best_alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search\n",
    "\n",
    "In reality, grid search is wasteful and not easy to control. A better (and still easy way) is to randomize the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, instead of specifying exact which points to test, we instead\n",
    "# have to specify a distribution to sample from.\n",
    "# For example, things from http://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import lognorm as sp_lognorm\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "parameters = {'polynomial_features__degree': sp_randint(1,20), # We want an integer\n",
    "              'linear_regression__alpha': sp_lognorm(1),\n",
    "              'polynomial_features__include_bias':[True, False]} # Selecting from two is fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need something whose logarithmic distribution we can control. How about a lognormal?\n",
    "$$\n",
    "\\mathcal{N}(\\ln x;\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left[-\\frac {(\\ln x - \\mu)^2} {2\\sigma^2}\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma=6\n",
    "rv = sp_lognorm(sigma,scale=1e-7)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(rv.rvs(size=1000),bins=np.logspace(-20, 2, 22))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'polynomial_features__degree': sp_randint(1,20), # We want an integer\n",
    "              'linear_regression__alpha': sp_lognorm(sigma,scale=1e-7),\n",
    "              'polynomial_features__include_bias':[True, False]} # Selecting from two is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the high degree polynomial makes the linear system almost\n",
    "# singular, which makes Numpy issue a Runtime warning.\n",
    "# This is not a problem here, except that it pops up the warning box\n",
    "# So I will disable it just for pedagogical purposes\n",
    "import warnings\n",
    "warnings.simplefilter('ignore',RuntimeWarning)\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# run randomized search\n",
    "#n_iter_search = 300 # How many random parameter settings should we try?\n",
    "n_iter_search = len(grid_search.cv_results_['params']) # Give it same # as grid search, to be fair\n",
    "random_search = RandomizedSearchCV(pipeline,\n",
    "                                   param_distributions=parameters,\n",
    "                                   n_iter=n_iter_search, \n",
    "                                   cv=shuffle_cv, # How do you want to cross-validate?\n",
    "                                   scoring = 'neg_mean_squared_error')\n",
    "random_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_ # Once finished, you can see what the best parameters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best MSE for Random Search: {:.2e}\".format(-random_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_degree = random_search.best_params_['polynomial_features__degree']\n",
    "best_alpha = random_search.best_params_['linear_regression__alpha']\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(X_plot, random_search.predict(X_plot),'-',label=\"Model\",alpha=0.5)\n",
    "plt.plot(X_plot, true_fun(X_plot), ':',label=\"True function\",alpha=1)\n",
    "plt.scatter(X,y, c='Blue', s=20, edgecolors='none')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "sns.despine()\n",
    "plt.title(\"Degree {}, Alpha {:.1e}\".format(best_degree,best_alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Bayesian Optimization\n",
    "\n",
    "Surely, since we are essentially doing optimization, we could approach hyper-parameter selection as an optimization problem as well, right?\n",
    "\n",
    "Enter techniques like Global Bayesian Optimization below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"The function to predict.\"\"\"\n",
    "    return x * np.sin(x)\n",
    "    # Try others!\n",
    "    #return 5 * np.sinc(x)\n",
    "    #return x\n",
    "    \n",
    "X = np.atleast_2d(np.linspace(0, 10, 200)).T\n",
    "\n",
    "# Observations\n",
    "y = f(X).ravel()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# This is just a helper function, no need to worry about\n",
    "# The internals.\n",
    "# We will return to this example in Week 14\n",
    "########################################################\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# Mesh the input space for evaluations of the real function, the prediction and\n",
    "# its MSE\n",
    "x = np.atleast_2d(np.linspace(0, 10, 1000)).T\n",
    "\n",
    "# Create a Gaussian Process model\n",
    "#kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
    "#gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "kernel = C(3.0)*RBF(1.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel,alpha=1e-6,optimizer=None)\n",
    "#gp = GaussianProcess(corr='cubic', theta0=1e-2, thetaL=1e-4, thetaU=1e-1,random_start=100)\n",
    "\n",
    "# Now, ready to begin learning:\n",
    "train_ind ={\n",
    "    'Upper CB':   np.zeros(len(X),dtype=bool),\n",
    "    'Random':np.zeros(len(X),dtype=bool)\n",
    "}\n",
    "options = train_ind.keys()\n",
    "\n",
    "possible_points = np.array(list(range(len(X))))\n",
    "# Possible Initialization options\n",
    "# 1. Select different points randomly\n",
    "#for i in range(2):\n",
    "#    for o in options:\n",
    "#        ind = np.random.choice(possible_points[~train_ind[o]],1)\n",
    "#        train_ind[o][ind] = True\n",
    "\n",
    "# 2. Start with end-points\n",
    "#for o in options:\n",
    "#    train_ind[o][0] = True\n",
    "#    train_ind[o][-1] = True\n",
    "\n",
    "# 3. Start with same random points\n",
    "for ind in np.random.choice(possible_points,2):\n",
    "    for o in options:\n",
    "        train_ind[o][ind] = True\n",
    "\n",
    "plot_list = np.array([5,10,20,30,40,50,len(X)])\n",
    "for i in range(10):\n",
    "    # As i increases, we increase the number of points\n",
    "    plt.figure(figsize=(16,6))\n",
    "    for j,o in enumerate(options):\n",
    "        plt.subplot(1,2,j+1)\n",
    "        gp.fit(X[train_ind[o],:],y[train_ind[o]])\n",
    "        yp,sigma = gp.predict(X[~train_ind[o],:], return_std=True)\n",
    "        ucb = yp + 1.96*sigma\n",
    "        if o == 'Upper CB':\n",
    "            #candidates = np.extract(MSE == np.amax(MSE),X[~train_ind[o],:])\n",
    "            candidates = np.extract(ucb == np.amax(ucb),X[~train_ind[o],:])\n",
    "            next_point = np.random.choice(candidates.flatten())\n",
    "            next_ind = np.argwhere(X.flatten() == next_point)\n",
    "        elif o == 'Random':\n",
    "            next_ind = np.random.choice(possible_points[~train_ind[o]],1)\n",
    "        train_ind[o][next_ind] = True\n",
    "        \n",
    "        # Plot intermediate results\n",
    "        yp,sigma = gp.predict(x, return_std=True)\n",
    "        plt.fill(np.concatenate([x, x[::-1]]),\n",
    "                np.concatenate([yp - 1.9600 * sigma,\n",
    "                               (yp + 1.9600 * sigma)[::-1]]),'b',\n",
    "                alpha=0.05,  ec='g', label='95% confidence interval')\n",
    "    \n",
    "        n_train = np.count_nonzero(train_ind[o])\n",
    "\n",
    "        gp.fit(X[train_ind[o],:],y[train_ind[o]])\n",
    "        # Show progress\n",
    "        yp,sigma = gp.predict(x, return_std=True)\n",
    "        yt = f(x)\n",
    "        error = np.linalg.norm(yp-yt.flatten())\n",
    "\n",
    "        plt.fill(np.concatenate([x, x[::-1]]),\n",
    "                np.concatenate([yp - 1.9600 * sigma,\n",
    "                               (yp + 1.9600 * sigma)[::-1]]),'b',\n",
    "                alpha=0.3,  ec='None', label='95% confidence interval')\n",
    "        \n",
    "        plt.plot(x,yt,'k--',alpha=1)\n",
    "        plt.plot(x,yp,'r-',alpha=1)\n",
    "        plt.scatter(X[train_ind[o],:],y[train_ind[o]],color='g',s=100)\n",
    "        plt.scatter(X[next_ind,:].flatten(),y[next_ind].flatten(),color='r',s=150)\n",
    "        plt.ylim([-10,15])\n",
    "        plt.xlim([0,10])\n",
    "        plt.title(\"%s\\n%d training points\\n%.2f error\"%(o,n_train,error))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n",
    "Discuss on board examples of the Curse of Dimensionality and how it affects algorithms dependent on calculating distances.\n",
    "\n",
    "* Space-filling properties of inscribed hyper-cube\n",
    "* Distance ratio between min and max distances\n",
    "* Effects on nearest neighbor graphs\n",
    "* Effects on Gaussian Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import gamma\n",
    "V_sphere = lambda d: np.pi**(d/2.0)\n",
    "V_cube = lambda d: d*2**(d-1)*gamma(d/2.0)\n",
    "volume_ratio = lambda d: V_sphere(d)/V_cube(d)\n",
    "\n",
    "d = range(2,50)\n",
    "ratio = [volume_ratio(i) for i in d]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(d,ratio)\n",
    "plt.semilogy(d,ratio)\n",
    "plt.ylabel(\"Ratio of Hyper-Sphere Vol. to Hyper-Cube Vol.\")\n",
    "plt.xlabel(\"Number of Dimensions\")\n",
    "plt.show()\n",
    "\n",
    "# TODO: Add distance min/max example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
