{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of AD on Verlet Integration\n",
    "This notebook demonstrates how to use Automatic Differentiation to determine the gradients of the initial conditions of a dynamical system (in this case a damped oscillator). To do this, we will define a numerical routine ([Verlet Integration](https://en.wikipedia.org/wiki/Verlet_integration)) and then use Automatic Differentiation to back propagate the gradient information from the output (Total system energy) to the initial conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from pylab import *\n",
    "import numpy as np\n",
    "\n",
    "###############################################################################\n",
    "N = 1000\n",
    "t = np.linspace(0,10,N)\n",
    "dt = t[1] - t[0]\n",
    "\n",
    "###############################################################################\n",
    "# functions\n",
    "def integrate(F,x0,v0,gamma):\n",
    "    ###########################################################################\n",
    "    # arrays are allocated and filled with zeros\n",
    "    #x = torch.tensor([0.0],requires_grad=True)\n",
    "    #v = torch.zeros(N)\n",
    "    Ef = 0\n",
    "    x = np.zeros(N)\n",
    "    v = np.zeros(N)\n",
    "    E = np.zeros(N)\n",
    "    \n",
    "    ###########################################################################    \n",
    "    # initial conditions\n",
    "    x[0] = x0\n",
    "    v[0] = v0\n",
    "    \n",
    "    ###########################################################################\n",
    "    # Do the Verlet Integration\n",
    "    fac1 = 1.0 - 0.5*gamma*dt\n",
    "    fac2 = 1.0/(1.0 + 0.5*gamma*dt)\n",
    "    \n",
    "    for i in range(N-1):\n",
    "        vn = fac1*fac2*v0 - fac2*dt*x0 + fac2*dt*F[i]\n",
    "        xn = x0 + dt*vn\n",
    "        Ef = 0.5*(x0**2 + ((v0 + vn)/2.0)**2)\n",
    "        v0 = vn\n",
    "        x0 = xn\n",
    "        # For Plotting/Debug\n",
    "\n",
    "        v[i + 1] = vn\n",
    "        x[i + 1] = xn\n",
    "        E[i] = Ef\n",
    "\n",
    "    Ef = 0.5*(x0**2 + v0**2)\n",
    "    \n",
    "    E[-1] = Ef\n",
    "    \n",
    "    ###########################################################################\n",
    "    # return solution\n",
    "    return ( (x0,v0,Ef) , (x,v,E) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from pylab import *\n",
    "import numpy as np\n",
    "\n",
    "###############################################################################\n",
    "N = 1000\n",
    "t = np.linspace(0,10,N)\n",
    "dt = t[1] - t[0]\n",
    "\n",
    "###############################################################################\n",
    "# functions\n",
    "def integrate(F,x0,v0,gamma):\n",
    "    ###########################################################################\n",
    "    # arrays are allocated and filled with zeros\n",
    "    #x = torch.tensor([0.0],requires_grad=True)\n",
    "    #v = torch.zeros(N)\n",
    "    Ef = torch.tensor([0.0],requires_grad=True)\n",
    "    x = np.zeros(N)\n",
    "    v = np.zeros(N)\n",
    "    E = np.zeros(N)\n",
    "    \n",
    "    ###########################################################################    \n",
    "    # initial conditions\n",
    "    with torch.no_grad():\n",
    "        x[0] = x0\n",
    "        v[0] = v0\n",
    "    \n",
    "    ###########################################################################\n",
    "    # Do the Verlet Integration\n",
    "    fac1 = 1.0 - 0.5*gamma*dt\n",
    "    fac2 = 1.0/(1.0 + 0.5*gamma*dt)\n",
    "    \n",
    "    for i in range(N-1):\n",
    "        vn = fac1*fac2*v0 - fac2*dt*x0 + fac2*dt*F[i]\n",
    "        xn = x0 + dt*vn\n",
    "        Ef = 0.5*(x0**2 + ((v0 + vn)/2.0)**2)\n",
    "        v0 = vn\n",
    "        x0 = xn\n",
    "        # For Plotting/Debug\n",
    "        with torch.no_grad():\n",
    "            v[i + 1] = vn\n",
    "            x[i + 1] = xn\n",
    "            E[i] = Ef\n",
    "    \n",
    "    Ef = 0.5*(x0**2 + v0**2)\n",
    "    with torch.no_grad():\n",
    "        E[-1] = Ef\n",
    "    \n",
    "    ###########################################################################\n",
    "    # return solution\n",
    "    return ( (x0,v0,Ef) , (x,v,E) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Do the actual numerical integration\n",
    "F = zeros(N)\n",
    "\n",
    "x_initial = torch.tensor([1.0], requires_grad = True)\n",
    "v_initial = torch.tensor([1.0], requires_grad = True)\n",
    "#gamma = torch.tensor([0.05], requires_grad = True)\n",
    "gamma = torch.tensor([.05], requires_grad = True)\n",
    "((xf,vf,Ef),(x1,v1,E1)) = integrate(F,x_initial,v_initial,gamma) # x0 = 0.0, v0 = 1.0, gamma = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#((),(x2,v2,E2)) = integrate(F,0.0,1.0,0.05) # x0 = 0.0, v0 = 1.0, gamma = 0.01\n",
    "#((),(x2,v2,E2)) = integrate(F,0.0,1.0,0.05) # x0 = 0.0, v0 = 1.0, gamma = 0.01\n",
    "\n",
    "#((),(x3,v3,E3)) = integrate(F,0.0,1.0,0.4) # x0 = 0.0, v0 = 1.0, gamma = 0.5\n",
    "\n",
    "###############################################################################\n",
    "def plot_solution(x1,E1,gamma):\n",
    "    rcParams[\"axes.grid\"] = True\n",
    "    rcParams['font.size'] = 14\n",
    "    rcParams['axes.labelsize'] = 18\n",
    "    figure()\n",
    "    subplot(211)\n",
    "    plot(t,x1)\n",
    "    #plot(t,x2)\n",
    "    #plot(t,x3)\n",
    "    ylabel(\"x(t)\")\n",
    "\n",
    "    subplot(212)\n",
    "    plot(t,E1,label=fr\"$\\gamma = {float(gamma):.2f}$\")\n",
    "    #plot(t,E2,label=r\"$\\gamma = 0.01$\")\n",
    "    #plot(t,E3,label=r\"$\\gamma = 0.5$\")\n",
    "    ylim(0,1.0)\n",
    "    ylabel(\"E(t)\")\n",
    "\n",
    "    xlabel(\"Time\")\n",
    "    legend(loc=\"center right\")\n",
    "\n",
    "    tight_layout()\n",
    "\n",
    "plot_solution(x1,E1,gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Ef)\n",
    "Ef.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print the gradient of the system Energy with respect to some of the initial conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gamma.grad)\n",
    "print(v_initial.grad)\n",
    "print(x_initial.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vf)\n",
    "vf.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gamma.grad)\n",
    "print(v_initial.grad)\n",
    "print(x_initial.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Damping Coefficient via SGD and AD\n",
    "\n",
    "First let's just get a visual intuition for how $\\gamma$ affects the final energy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gammas = 30\n",
    "gamma_plot = np.logspace(-0.5,1.0,num_gammas)\n",
    "Efs = np.zeros(num_gammas)\n",
    "for i,g in enumerate(gamma_plot):\n",
    "    ((xf,vf,Ef),(x1,v1,E1)) = integrate(F,x_initial,v_initial,g)\n",
    "    Efs[i] = Ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilogx(gamma_plot,Efs)\n",
    "xlabel(r'$\\gamma$')\n",
    "ylabel('E Final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a pretty flat plateau from around $\\gamma=1$ until around $\\gamma=3$.\n",
    "\n",
    "Now let's use our backward mode AD to actually optimize $\\gamma$ directly by calling backward on the output of the final energy of the Verlet integration of the ODE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is just a helper library for plotting\n",
    "def plot_optimization(initial_gamma, num_steps, optimizer, opt_kwargs={}):\n",
    "    # Take an initial guess at the optimum:\n",
    "    gamma = torch.tensor([initial_gamma], requires_grad=True)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = optimizer([gamma], **opt_kwargs)\n",
    "\n",
    "    steps = [ ] # Here is where we'll keep track of the steps\n",
    "    # Take num_steps of the optimizer\n",
    "    for i in range(num_steps):\n",
    "        # This function runs an actual optimization step. We wrap it in closure so that optimizers\n",
    "        # that take multiple function calls per step can do so -- e.g., LBFGS.\n",
    "        def closure():\n",
    "            # Get rid of the existing gradients on the tape\n",
    "            optimizer.zero_grad()\n",
    "            # Run the numerical integration -- this is the forward pass through the solver\n",
    "            ((xf,vf,Ef),(x1,v1,E1)) = integrate(F,x_initial,v_initial,gamma) # x0 = 0.0, v0 = 1.0, gamma = 0.0\n",
    "            # Compute the backward mode AD pass\n",
    "            Ef.backward()\n",
    "            return Ef\n",
    "        # Now ask the optimizer to take a step\n",
    "        optimizer.step(closure)\n",
    "        \n",
    "        # The below part is just for printing/plotting. We call torch.no_grad() here to signify that\n",
    "        # we do not need to track this as part of the gradient operations. That is, these parts will not\n",
    "        # be added to the computational graph or used for backward mode AD.\n",
    "        with torch.no_grad():\n",
    "            #print(gamma)\n",
    "            # Run again just to plot the solution for this gamma\n",
    "            ((xf,vf,Ef),(x1,v1,E1)) = integrate(F,x_initial,v_initial,gamma)\n",
    "            #print(Ef)\n",
    "            if num_steps>10 and i%3==0:\n",
    "                plot_solution(x1,E1,gamma)\n",
    "            # Add it to steps so that we can see/plot it later.\n",
    "            steps.append(np.array(gamma.detach().numpy()))\n",
    "            \n",
    "    steps = np.array(steps)\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAM Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_Adam = plot_optimization(0.05, 20,\n",
    "                              torch.optim.AdamW,\n",
    "                              {'lr':0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_SGD = plot_optimization(0.05, 20,\n",
    "                              torch.optim.SGD,\n",
    "                              {'lr':0.05,'momentum':0.9})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LBFGS Example\n",
    "(Warning: Per-run solves of LBFGS take a while, so don't set num_steps too high here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_LBFGS = plot_optimization(0.05, 5,\n",
    "                              torch.optim.LBFGS,\n",
    "                             {'lr':0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the steps taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilogx(gamma_plot,Efs)\n",
    "steps_Adam = steps_Adam.flatten()\n",
    "plot(steps_Adam,[0.0]*len(steps_Adam),'|', color = 'r', label = 'Adam Steps')\n",
    "plot(steps_SGD,[0.005]*len(steps_SGD),'|', color = 'g', label = 'SGD Steps')\n",
    "plot(steps_LBFGS,[0.01]*len(steps_LBFGS),'|', color = 'k', label = 'LBFGS Steps')\n",
    "xlabel(r'$\\gamma$')\n",
    "ylabel('E Final')\n",
    "legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4me",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
