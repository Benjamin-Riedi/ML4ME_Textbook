{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Power Electronics Challenge\n",
    "\n",
    "This one is an open challenge, as even we (the research group) struggle with it. A description of the problem in the [EngiBench documentation](https://engibench.ethz.ch/problems/power_electronics/). The dataset can be found on [HuggingFace](https://huggingface.co/datasets/IDEALLab/power_electronics_v0).\n",
    "\n",
    "If you manage to make it learn correctly for both DC Gain and Voltage Ripple, please do reach out to Florian Felten: ffelten@mavt.ethz.ch.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from engibench.problems.power_electronics.v0 import PowerElectronics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeds\n",
    "my_seed = 42\n",
    "\n",
    "torch.manual_seed(my_seed)  # PyTorch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.benchmark = False\n",
    "rng = np.random.default_rng(my_seed)  # NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Dataset loading and Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = PowerElectronics()\n",
    "XY_train = problem.dataset[\"train\"].with_format(\"numpy\")\n",
    "XY_test = problem.dataset[\"test\"].with_format(\"numpy\")\n",
    "\n",
    "X_train = np.array(XY_train[\"initial_design\"])\n",
    "y_train = np.array([XY_train[\"Voltage_Ripple\"], XY_train[\"DcGain\"]]).T\n",
    "\n",
    "X_test = np.array(XY_test[\"initial_design\"])\n",
    "y_test = np.array([XY_test[\"Voltage_Ripple\"], XY_test[\"DcGain\"]]).T\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make two subplots\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "# plot histogram of voltage ripple\n",
    "axs[0].hist(y_train[:, 0])\n",
    "# plot histogram of dc gain\n",
    "axs[1].hist(y_train[:, 1])\n",
    "axs[0].set_title(\"Voltage Ripple\")\n",
    "axs[1].set_title(\"Dc Gain\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "# Normalizing our data, don't forget to unnormalize the predictions later!\n",
    "X_train_scaled = torch.from_numpy(x_scaler.fit_transform(X_train)).float()\n",
    "X_test_scaled = torch.from_numpy(x_scaler.transform(X_test)).float()\n",
    "y_train_scaled = torch.from_numpy(y_scaler.fit_transform(y_train)).float()\n",
    "y_test_scaled = torch.from_numpy(y_scaler.transform(y_test)).float()\n",
    "\n",
    "\n",
    "# make two subplots\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "# plot histogram of voltage ripple\n",
    "axs[0].hist(y_train_scaled[:, 0])\n",
    "# plot histogram of dc gain\n",
    "axs[1].hist(y_train_scaled[:, 1])\n",
    "axs[0].set_title(\"Voltage Ripple (scaled)\")\n",
    "axs[1].set_title(\"Dc Gain (scaled)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class NetleF(nn.Module):\n",
    "    \"\"\"A very efficient neural network for the given task, probably the best for the job.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(20, 50)) # input layer\n",
    "        for _ in range(5):\n",
    "            layers.append(nn.ReLU()) # activation function\n",
    "            layers.append(nn.Linear(50, 50)) # hidden layer\n",
    "        layers.append(nn.ReLU()) # activation function\n",
    "        layers.append(nn.Linear(50, 2)) # output layer\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetleF()\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(800):\n",
    "    y_pred_scaled = model(X_train_scaled).squeeze()\n",
    "    loss = loss_fn(y_pred_scaled, y_train_scaled)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        with torch.no_grad(): # no gradients coming from the test set\n",
    "            test_pred_scaled = model(X_test_scaled).squeeze()\n",
    "            test_loss = loss_fn(test_pred_scaled, y_test_scaled)\n",
    "            train_losses.append(loss.item())\n",
    "            test_losses.append(test_loss.item())\n",
    "        print(f\"Epoch {epoch:3d} | Train loss: {loss.item():.4f} | Test loss: {test_loss.item():.4f}\")\n",
    "\n",
    "# plot train and test losses\n",
    "plt.plot(train_losses, label=\"Train loss\")\n",
    "plt.plot(test_losses, label=\"Test loss\")\n",
    "plt.xlabel(\"Epoch // 5\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train and Test Losses\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several predictions\n",
    "with torch.no_grad():\n",
    "    test_pred_scaled = model(X_test_scaled)\n",
    "    # unscale the predictions\n",
    "    test_pred = torch.from_numpy(y_scaler.inverse_transform(test_pred_scaled.numpy())).float()\n",
    "\n",
    "    # 5 random predictions\n",
    "    random_indices = np.random.randint(0, len(test_pred), size=3)\n",
    "    for i in random_indices:\n",
    "        voltage_ripple_prediction = test_pred[i][0].item()\n",
    "        voltage_ripple_target = y_test[i][0]\n",
    "        voltage_ripple_error = voltage_ripple_target - voltage_ripple_prediction\n",
    "        voltage_ripple_and_error = \"Voltage Ripple:\\n\\t prediction:\" + str(voltage_ripple_prediction) + \"\\n\\t target:\" + str(voltage_ripple_target) + \"\\n\\t error:\" + str(voltage_ripple_error)\n",
    "\n",
    "        dc_gain_prediction = test_pred[i][1].item()\n",
    "        dc_gain_target = y_test[i][1]\n",
    "        dc_gain_error = dc_gain_target - dc_gain_prediction\n",
    "        dc_gain_and_error = \"Dc Gain:\\n\\t prediction:\" + str(dc_gain_prediction) + \"\\n\\t target:\" + str(dc_gain_target) + \"\\n\\t error:\" + str(dc_gain_error)\n",
    "\n",
    "        print(voltage_ripple_and_error)\n",
    "        print(dc_gain_and_error)\n",
    "        print(\"--------------------------------\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Voltage Ripple\n",
    "axs[0].scatter(y_test[:,0], test_pred[:,0])\n",
    "# axs[0].plot([y_test[:,0].min(), y_test[:,0].max()],\n",
    "#            [y_test[:,0].min(), y_test[:,0].max()], \"k--\", alpha=0.5)\n",
    "axs[0].set_xlabel(\"Target Voltage Ripple\")\n",
    "axs[0].set_ylabel(\"Predicted Voltage Ripple\")\n",
    "axs[0].set_title(\"Voltage Ripple Predictions\")\n",
    "\n",
    "# DC Gain\n",
    "axs[1].scatter(y_test[:,1], test_pred[:,1])\n",
    "# axs[1].plot([y_test[:,1].min(), y_test[:,1].max()],\n",
    "#            [y_test[:,1].min(), y_test[:,1].max()], \"k--\", alpha=0.5)\n",
    "axs[1].set_xlabel(\"Target Dc Gain\")\n",
    "axs[1].set_ylabel(\"Predicted Dc Gain\")\n",
    "axs[1].set_title(\"DC Gain Predictions\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_np = test_pred.numpy() if torch.is_tensor(test_pred) else test_pred\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axs[0].hist(y_test[:,0] - test_pred_np[:,0], bins=40)\n",
    "axs[1].hist(y_test[:,1] - test_pred_np[:,1], bins=40)\n",
    "axs[0].set_title(\"Voltage Ripple Error Distribution\")\n",
    "axs[1].set_title(\"DC Gain Error Distribution\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
