{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of Derivative Free Optimization Methods\n",
    "-------\n",
    "\n",
    "Now that we have introduce the usage of hyper-parameters and cross-validation, a natural question arises: How do we choose the hyper-parameters? There are many ways to do this, and this section will describe the most common and basic ones, while leaving more advanced techniques (like Implicit Differentiation) for later. Specifically, this section will:\n",
    "1. Define the concepts of Grid and Random Hyper-parameter search.\n",
    "2. Use Grid and Random search to optimize hyper-parameters of a model.\n",
    "2. Distinguish when Randomized Search is much better than grid search.\n",
    "3. Describe how Global Optimization procedures such as Bayesian Optimization work.\n",
    "4. Recognize why none of those at all work in High Dimensions and describe the \"Curse of Dimensionality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set_style(\"white\")\n",
    "pal = sns.color_palette(\"Paired\")\n",
    "cmap = sns.blend_palette(pal,as_cmap=True)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 30\n",
    "\n",
    "# True Function we want to estimate\n",
    "true_fun = lambda X: np.cos(1.5 * np.pi * X)\n",
    "\n",
    "# Noisy Samples from the true function\n",
    "X = np.sort(np.random.rand(n_samples))[:, np.newaxis]\n",
    "y = true_fun(X) + np.random.randn(n_samples)[:, np.newaxis] * 0.1\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "# Plot the true function:\n",
    "X_plot = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "plt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n",
    "# Plot the data samples\n",
    "plt.scatter(X,y, label=\"Samples\")\n",
    "plt.legend(loc=\"best\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Data using Polynomial Regression\n",
    "Unlike where we used K Nearest Neighbors, this time we will fit the data using Polynomial Regression ($y=a+bx+cx^2 + \\cdots$)\n",
    "\n",
    "In this particular case, we will use something called \"Ridge regression\" though the specific details of what this is and why it works we will get into next week. For today, the key things to note are that the models we build below have two knobs or \"hyper-parameters\" that dictate model complexity:\n",
    "\n",
    "1. The degree of the polynomial curve fit. That is d=1 corresponds to $f(x) = w_1\\cdot x^1 = w_1\\cdot x$. d=2 corresponds to $f(x) = w_1\\cdot x^1 + w_2\\cdot x^2$, and d=D corresponds to $f(x) = w_1\\cdot x^1 + w_2\\cdot x^2 + \\cdots + w_D\\cdot x^D$. (Note that you could also have an intercept term such as $w_0 + w_1\\cdot x$, but for the below example we have disabled this addition.)\n",
    "2. A penalty term called $\\alpha$ (more on this next week) which essentially penalizes large weights ($w_1,\\cdots, w_D$). When $\\alpha$ is small, the weights can be whatever they want (high complexity function), and when $\\alpha$ is high, the weights have to be close to zero (low complexity function--e.g., just a flat line)\n",
    "\n",
    "What we are going to see today is how to pick $d$ and $\\alpha$ such that we maximize cross-validation performance. Next week we will dive more into the details of various linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "# Let's plot the behavior of a fixed degree polynomial\n",
    "degree = 15\n",
    "# (i.e., f(x) = w_1*x + w_2*x^2 + ... + w_15*x^15)\n",
    "# but where we change alpha.\n",
    "alphas = np.logspace(start=-13,stop=4,num=20)\n",
    "polynomial_features = PolynomialFeatures(degree=degree,\n",
    "                                         include_bias=False)\n",
    "scores = []\n",
    "for a in alphas:\n",
    "    linear_regression = Ridge(alpha=a)\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    cv_scores = model_selection.cross_val_score(pipeline,\n",
    "        X, y, scoring=\"neg_mean_squared_error\", cv=20)\n",
    "    scores.append(cv_scores)\n",
    "\n",
    "scores = np.array(scores)\n",
    "\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.semilogx(alphas,-np.mean(scores,axis=1),'-')\n",
    "plt.ylabel('Test MSE')\n",
    "plt.xlabel('Alpha ($\\\\alpha$)')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we have more than one variable?\n",
    "Let's look at both polynomial degree and regularization weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "alphas = np.logspace(start=-13, # Start at 1e-13\n",
    "                     stop=4,    # Stop at 1e4\n",
    "                     num=40)    # Split that into 40 pieces\n",
    "degrees = range(1,16) # This will only go to 15, due to how range works\n",
    "\n",
    "scores = np.zeros(shape=(len(degrees), # i.e., 15\n",
    "                         len(alphas))) # i.e., 20\n",
    "\n",
    "for i, degree in enumerate(degrees): # For each degree\n",
    "    polynomial_features = PolynomialFeatures(degree=degree,\n",
    "                                             include_bias=False)\n",
    "    \n",
    "    for j,a in enumerate(alphas):    # For each alpha\n",
    "        linear_regression = Ridge(alpha=a)\n",
    "        pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                             (\"linear_regression\", linear_regression)])\n",
    "        cv_scores = model_selection.cross_val_score(pipeline,\n",
    "            X, y, scoring=\"neg_mean_squared_error\", cv=20)\n",
    "        scores[i][j] = -np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "Xs, Ys = np.meshgrid(range(len(degrees)), range(len(alphas)))\n",
    "zs = np.array([scores[i,j] for i,j in zip(np.ravel(Xs), np.ravel(Ys))])\n",
    "Zs = zs.reshape(Xs.shape)\n",
    "\n",
    "Xs, Ys = np.meshgrid(degrees, np.log(alphas))\n",
    "\n",
    "ax.plot_surface(Xs, Ys, Zs, rstride=1, cstride=1, cmap=cm.coolwarm,\n",
    "    linewidth=0, antialiased=False)\n",
    "\n",
    "# Label the Axes\n",
    "ax.set_xlabel('Degree')\n",
    "ax.set_ylabel('Regularization')\n",
    "ax.set_zlabel('MSE')\n",
    "\n",
    "# Rotate the image\n",
    "ax.view_init(30, # larger # goes \"higher\"\n",
    "             30) # larger # \"circles around\"\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,10))\n",
    "plt.imshow(Zs,\n",
    "           cmap=cm.coolwarm, # Allows you to set the color\n",
    "           vmin=Zs.min(), vmax=0.2, # The min and max Z-Values (for coloring purposes)\n",
    "           extent=[Xs.min(), Xs.max(),   # How far on X-Axis you want to plot\n",
    "                   Ys.min(), Ys.max()],  # How far on Y-Axis\n",
    "           interpolation='spline16',      # How do you want to interpolate values between data?\n",
    "           origin='lower')\n",
    "plt.title('Mean Squared Error')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Regularization')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "At the end of the day, all this is doing is optimization/search over different parameters.\n",
    "\n",
    "How should we go about automating this?\n",
    "\n",
    "Most common: Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('parameters we could change:')\n",
    "for k in pipeline.get_params().keys():\n",
    "    print(\" \",k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'polynomial_features__degree': list(range(1,16)), # 15 possible\n",
    "              'linear_regression__alpha': np.logspace(start=-13,stop=4,num=10),\n",
    "              'polynomial_features__include_bias':['True', 'False']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we want to do cross-validation?\n",
    "from sklearn import model_selection\n",
    "num_data_points = len(y)\n",
    "\n",
    "# 4-fold CV\n",
    "kfold_cv = model_selection.KFold(n_splits = 4) \n",
    "\n",
    "# Or maybe you want randomized splits?\n",
    "shuffle_cv = model_selection.ShuffleSplit(n_splits = 20,     # How many iterations?\n",
    "                                          test_size=0.2    # What % should we keep for test?\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline,    # The thing we want to optimize\n",
    "                           parameters,  # The parameters we will change\n",
    "                           cv=shuffle_cv, # How do you want to cross-validate?\n",
    "                           scoring = 'neg_mean_squared_error'\n",
    "                          )\n",
    "grid_search.fit(X, y) # This runs the cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_ # Once finished, you can see what the best parameters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best MSE for Grid Search: {:.2e}\".format(-grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.predict(X)  # You can also use the best model directly (in sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_degree = grid_search.best_params_['polynomial_features__degree']\n",
    "best_alpha = grid_search.best_params_['linear_regression__alpha']\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(X_plot, grid_search.predict(X_plot),'-',label=\"Model\",alpha=0.5)\n",
    "plt.plot(X_plot, true_fun(X_plot), ':',label=\"True function\",alpha=1)\n",
    "plt.scatter(X,y, c='Blue', s=20, edgecolors='none')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "sns.despine()\n",
    "plt.title(\"Degree {}, Alpha {:.1e}\".format(best_degree,best_alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search\n",
    "\n",
    "In reality, grid search is wasteful and not easy to control. A better (and still easy way) is to randomize the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, instead of specifying exact which points to test, we instead\n",
    "# have to specify a distribution to sample from.\n",
    "# For example, things from http://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import lognorm as sp_lognorm\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "parameters = {'polynomial_features__degree': sp_randint(1,20), # We want an integer\n",
    "              'linear_regression__alpha': sp_lognorm(1),\n",
    "              'polynomial_features__include_bias':['True', 'False']} # Selecting from two is fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need something whose logarithmic distribution we can control. How about a lognormal?\n",
    "$$\n",
    "\\mathcal{N}(\\ln x;\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left[-\\frac {(\\ln x - \\mu)^2} {2\\sigma^2}\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma=6\n",
    "rv = sp_lognorm(sigma,scale=1e-7)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(rv.rvs(size=1000),bins=np.logspace(-20, 2, 22))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'polynomial_features__degree': sp_randint(1,20), # We want an integer\n",
    "              'linear_regression__alpha': sp_lognorm(sigma,scale=1e-7),\n",
    "              'polynomial_features__include_bias':['True', 'False']} # Selecting from two is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the high degree polynomial makes the linear system almost\n",
    "# singular, which makes Numpy issue a Runtime warning.\n",
    "# This is not a problem here, except that it pops up the warning box\n",
    "# So I will disable it just for pedagogical purposes\n",
    "import warnings\n",
    "warnings.simplefilter('ignore',RuntimeWarning)\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# run randomized search\n",
    "#n_iter_search = 300 # How many random parameter settings should we try?\n",
    "n_iter_search = len(grid_search.cv_results_['params']) # Give it same # as grid search, to be fair\n",
    "random_search = RandomizedSearchCV(pipeline,\n",
    "                                   param_distributions=parameters,\n",
    "                                   n_iter=n_iter_search, \n",
    "                                   cv=shuffle_cv, # How do you want to cross-validate?\n",
    "                                   scoring = 'neg_mean_squared_error')\n",
    "random_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_ # Once finished, you can see what the best parameters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best MSE for Random Search: {:.2e}\".format(-random_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_degree = random_search.best_params_['polynomial_features__degree']\n",
    "best_alpha = random_search.best_params_['linear_regression__alpha']\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(X_plot, random_search.predict(X_plot),'-',label=\"Model\",alpha=0.5)\n",
    "plt.plot(X_plot, true_fun(X_plot), ':',label=\"True function\",alpha=1)\n",
    "plt.scatter(X,y, c='Blue', s=20, edgecolors='none')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "sns.despine()\n",
    "plt.title(\"Degree {}, Alpha {:.1e}\".format(best_degree,best_alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Bayesian Optimization\n",
    "\n",
    "Surely, since we are essentially doing optimization, we could approach hyper-parameter selection as an optimization problem as well, right?\n",
    "\n",
    "Enter techniques like Global Bayesian Optimization below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"The function to predict.\"\"\"\n",
    "    return x * np.sin(x)\n",
    "    # Try others!\n",
    "    #return 5 * np.sinc(x)\n",
    "    #return x\n",
    "    \n",
    "X = np.atleast_2d(np.linspace(0, 10, 200)).T\n",
    "\n",
    "# Observations\n",
    "y = f(X).ravel()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# This is just a helper function, no need to worry about\n",
    "# The internals.\n",
    "# We will return to this example in Week 14\n",
    "########################################################\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# Mesh the input space for evaluations of the real function, the prediction and\n",
    "# its MSE\n",
    "x = np.atleast_2d(np.linspace(0, 10, 1000)).T\n",
    "\n",
    "# Create a Gaussian Process model\n",
    "#kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
    "#gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "kernel = C(3.0)*RBF(1.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel,alpha=1e-6,optimizer=None)\n",
    "#gp = GaussianProcess(corr='cubic', theta0=1e-2, thetaL=1e-4, thetaU=1e-1,random_start=100)\n",
    "\n",
    "# Now, ready to begin learning:\n",
    "train_ind ={\n",
    "    'Upper CB':   np.zeros(len(X),dtype=bool),\n",
    "    'Random':np.zeros(len(X),dtype=bool)\n",
    "}\n",
    "options = train_ind.keys()\n",
    "\n",
    "possible_points = np.array(list(range(len(X))))\n",
    "# Possible Initialization options\n",
    "# 1. Select different points randomly\n",
    "#for i in range(2):\n",
    "#    for o in options:\n",
    "#        ind = np.random.choice(possible_points[~train_ind[o]],1)\n",
    "#        train_ind[o][ind] = True\n",
    "\n",
    "# 2. Start with end-points\n",
    "#for o in options:\n",
    "#    train_ind[o][0] = True\n",
    "#    train_ind[o][-1] = True\n",
    "\n",
    "# 3. Start with same random points\n",
    "for ind in np.random.choice(possible_points,2):\n",
    "    for o in options:\n",
    "        train_ind[o][ind] = True\n",
    "\n",
    "plot_list = np.array([5,10,20,30,40,50,len(X)])\n",
    "for i in range(10):\n",
    "    # As i increases, we increase the number of points\n",
    "    plt.figure(figsize=(16,6))\n",
    "    for j,o in enumerate(options):\n",
    "        plt.subplot(1,2,j+1)\n",
    "        gp.fit(X[train_ind[o],:],y[train_ind[o]])\n",
    "        yp,sigma = gp.predict(X[~train_ind[o],:], return_std=True)\n",
    "        ucb = yp + 1.96*sigma\n",
    "        if o is 'Upper CB':\n",
    "            #candidates = np.extract(MSE == np.amax(MSE),X[~train_ind[o],:])\n",
    "            candidates = np.extract(ucb == np.amax(ucb),X[~train_ind[o],:])\n",
    "            next_point = np.random.choice(candidates.flatten())\n",
    "            next_ind = np.argwhere(X.flatten() == next_point)\n",
    "        elif o is 'Random':\n",
    "            next_ind = np.random.choice(possible_points[~train_ind[o]],1)\n",
    "        train_ind[o][next_ind] = True\n",
    "        \n",
    "        # Plot intermediate results\n",
    "        yp,sigma = gp.predict(x, return_std=True)\n",
    "        plt.fill(np.concatenate([x, x[::-1]]),\n",
    "                np.concatenate([yp - 1.9600 * sigma,\n",
    "                               (yp + 1.9600 * sigma)[::-1]]),'b',\n",
    "                alpha=0.05,  ec='g', label='95% confidence interval')\n",
    "    \n",
    "        n_train = np.count_nonzero(train_ind[o])\n",
    "\n",
    "        gp.fit(X[train_ind[o],:],y[train_ind[o]])\n",
    "        # Show progress\n",
    "        yp,sigma = gp.predict(x, return_std=True)\n",
    "        yt = f(x)\n",
    "        error = np.linalg.norm(yp-yt.flatten())\n",
    "\n",
    "        plt.fill(np.concatenate([x, x[::-1]]),\n",
    "                np.concatenate([yp - 1.9600 * sigma,\n",
    "                               (yp + 1.9600 * sigma)[::-1]]),'b',\n",
    "                alpha=0.3,  ec='None', label='95% confidence interval')\n",
    "        \n",
    "        plt.plot(x,yt,'k--',alpha=1)\n",
    "        plt.plot(x,yp,'r-',alpha=1)\n",
    "        plt.scatter(X[train_ind[o],:],y[train_ind[o]],color='g',s=100)\n",
    "        plt.scatter(X[next_ind,:].flatten(),y[next_ind].flatten(),color='r',s=150)\n",
    "        plt.ylim([-10,15])\n",
    "        plt.xlim([0,10])\n",
    "        plt.title(\"%s\\n%d training points\\n%.2f error\"%(o,n_train,error))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n",
    "Discuss on board examples of the Curse of Dimensionality and how it affects algorithms dependent on calculating distances.\n",
    "\n",
    "* Space-filling properties of inscribed hyper-cube\n",
    "* Distance ratio between min and max distances\n",
    "* Effects on nearest neighbor graphs\n",
    "* Effects on Gaussian Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import gamma\n",
    "V_sphere = lambda d: np.pi**(d/2.0)\n",
    "V_cube = lambda d: d*2**(d-1)*gamma(d/2.0)\n",
    "volume_ratio = lambda d: V_sphere(d)/V_cube(d)\n",
    "\n",
    "d = range(2,50)\n",
    "ratio = [volume_ratio(i) for i in d]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(d,ratio)\n",
    "plt.semilogy(d,ratio)\n",
    "plt.ylabel(\"Ratio of Hyper-Sphere Vol. to Hyper-Cube Vol.\")\n",
    "plt.xlabel(\"Number of Dimensions\")\n",
    "plt.show()\n",
    "\n",
    "# TODO: Add distance min/max example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
