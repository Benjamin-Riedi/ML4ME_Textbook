{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: GAN Training Pitfalls\n",
    "author: Mark Fuge\n",
    "date: 'October 12 2025'\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "        code-summary: \"Show Code\"\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports (shared utilities)\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# get the location of this file\n",
    "main_fpath = os.path.abspath('')\n",
    "os.chdir(main_fpath)\n",
    "\n",
    "# Optional interactive widgets\n",
    "try:\n",
    "    from ipywidgets import interact, FloatSlider, IntSlider\n",
    "except Exception:\n",
    "    interact = None\n",
    "    FloatSlider = IntSlider = None\n",
    "\n",
    "# Import shared utilities from the local module\n",
    "from gen_models_utilities import (\n",
    "    RNG_SEED, device, _compute_axis_limits, create_ring_gaussians, \n",
    "    GanHistory, make_loader,\n",
    "    compute_diversity_metric, plot_model_diagnostics, plot_latent_interpolation\n",
    ")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "This notebook will highlight some of the original problems with GAN models, some of the techniques you can use to diagnose model errors, and also introduce common rememdies for simple feed-forward GAN models. The next notebook will introduce the concept of Optimal Transport, and show how this mitigates some of the problems caused by the original GAN formulation as a minimax game.\n",
    "\n",
    "As with the last notebook, we will use the simple Ring of Gaussian Toy example to illustrate this for pedagogical purposes. In later notebooks, we will see how these problems can manifest in more complex settings.\n",
    "\n",
    "Let's start by loading the dataset again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ring, y_ring = create_ring_gaussians()\n",
    "plt.figure(figsize=(5.5,5.5))\n",
    "sc = plt.scatter(X_ring[:,0], X_ring[:,1], c=y_ring, cmap='tab10', s=10, alpha=0.6)\n",
    "plt.colorbar(sc, label='Mode index')\n",
    "plt.title('Toy Dataset: Colored Gaussian Ring')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ring_latent_limits = ((-3.5, 3.5), (-3.5, 3.5))\n",
    "ring_data_limits = _compute_axis_limits(X_ring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "We will now run a series of experiments to show you common problems and how they manifest in the plotting visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Experiment 1: When the Discriminator Overpowers the Generator\n",
    "\n",
    "In this experiment, we will see what happens with the Discriminator part of the GAN significantly overpowers the Generator. This can occur because of several reasons:\n",
    "\n",
    "1. The Discriminator is a much larger (or higher capacity) network than the Generator, and thus is able to move its classification decision boundary in more complex ways to distinguish real from fake samples, and the Generator doesn't have sufficient capacity to compensate for this.\n",
    "2. The Discriminator trains much faster than the Generator, and thus can move its decision boundary faster than the Generator can keep up.\n",
    "3. The Discriminator is initialized (either on purpose or by random chance) such that it has an initial significant advantage over the Generator. In this case, even if the Generator has high capacity and can train quickly, it may not be able to recover from this initial disadvantage, as the discriminator make lock it into a portion of the sample space from which local gradient descent on the Generator cannot recover.\n",
    "\n",
    "We can induce this below by just significantly increasing the learning rate of the Discriminator relative to the Generator. Consider the following questions and experiment:\n",
    "\n",
    "::: {.callout-tip appearance=\"default\"}\n",
    "### Experiment: What happens when the Discriminator overpowers the Generator?\n",
    "Below you can change the relative learning rates of the Discriminator versus the Generator:\n",
    "\n",
    "- What happens when the Discriminator is able to train significantly faster than the Generator? What about the other way around?\n",
    "- How can we tell from the various loss plots that the Discriminator is overpowering the Generator? How does this behavior manifest itself in the generated samples?\n",
    "- Why is it that the generator is unable to recover from this situation, even though it has the capacity to represent the data distribution? What about the training procedure and dynamics of the minimax game prevents the Generator from catching up?\n",
    "- In the latent interpolation plots, as the GAN spans its latent set of coordinates, what happens to the generated samples?\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPGenerator(nn.Module):\n",
    "    def __init__(self, noise_dim: int = 2, hidden_dim: int = 256, out_dim: int = 2, spectral_normalization: bool = False):\n",
    "        super().__init__()\n",
    "        # Create linear layers and optionally apply spectral normalization\n",
    "        lin1 = nn.Linear(noise_dim, hidden_dim)\n",
    "        lin2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        lin3 = nn.Linear(hidden_dim, out_dim)\n",
    "        if spectral_normalization:\n",
    "            from torch.nn.utils import spectral_norm\n",
    "            lin1 = spectral_norm(lin1)\n",
    "            lin2 = spectral_norm(lin2)\n",
    "            lin3 = spectral_norm(lin3)\n",
    "        self.main = nn.Sequential(\n",
    "            lin1, nn.LeakyReLU(),\n",
    "            lin2, nn.LeakyReLU(),\n",
    "            lin3,\n",
    "        )\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.main(z)\n",
    "\n",
    "class MLPDiscriminator(nn.Module):\n",
    "    def __init__(self, input_dim: int = 2, hidden_dim: int = 256, spectral_normalization: bool = False):\n",
    "        super().__init__()\n",
    "        # Optionally apply spectral normalization to linear layers to stabilize training\n",
    "        layers = []\n",
    "        lin1 = nn.Linear(input_dim, hidden_dim)\n",
    "        lin2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        lin3 = nn.Linear(hidden_dim, 1)\n",
    "        if spectral_normalization:\n",
    "            from torch.nn.utils import spectral_norm\n",
    "            lin1 = spectral_norm(lin1)\n",
    "            lin2 = spectral_norm(lin2)\n",
    "            lin3 = spectral_norm(lin3)\n",
    "        layers.extend([lin1, nn.LeakyReLU(0.2), lin2, nn.LeakyReLU(0.2), lin3])\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.main(x).squeeze(-1)\n",
    "\n",
    "\n",
    "def build_generator(noise_dim=2, hidden_dim=256, spectral_normalization: bool = False):\n",
    "    return MLPGenerator(noise_dim=noise_dim, hidden_dim=hidden_dim, spectral_normalization=spectral_normalization).to(device)\n",
    "\n",
    "\n",
    "def build_discriminator(hidden_dim=256, spectral_normalization: bool = False):\n",
    "    return MLPDiscriminator(hidden_dim=hidden_dim, spectral_normalization=spectral_normalization).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "def train_vanilla_gan(\n",
    "    data: np.ndarray, *, noise_dim: int = 2, batch_size: int = 256, epochs: int = 120,\n",
    "    lr_g: float = 2e-4, lr_d: float = 2e-4, hidden_dim: int = 256, print_every: int = 40,\n",
    "    spectral_normalization: bool = False, checkpoint_interval: int = 0) -> tuple[nn.Module, nn.Module, GanHistory]:\n",
    "    # Load the data into a DataLoader for batching and make PyTorch happy\n",
    "    loader = make_loader(data, batch_size)\n",
    "\n",
    "    # Set up the basic networks\n",
    "    G = build_generator(noise_dim=noise_dim, hidden_dim=hidden_dim)\n",
    "    D = build_discriminator(hidden_dim=hidden_dim, spectral_normalization=spectral_normalization)\n",
    "    # Instantiate the optimizers for each model\n",
    "    opt_g = optim.Adam(G.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "    opt_d = optim.Adam(D.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    # Record the loss history for plotting later\n",
    "    hist = GanHistory([], [], [], [], [])\n",
    "\n",
    "    # Helper to capture a snapshot\n",
    "    def _capture_snapshot(ep_idx: int):\n",
    "        with torch.no_grad():\n",
    "            z_eval = torch.randn(2048, noise_dim, device=device)\n",
    "            samples = G(z_eval).cpu().numpy()\n",
    "        # Save a shallow copy of generator state and a small sample for visualization\n",
    "        state = {k: v.cpu().clone() for k, v in G.state_dict().items()}\n",
    "        small_samples = samples[np.random.choice(samples.shape[0], size=min(512, samples.shape[0]), replace=False)]\n",
    "        hist.snapshots.append({\"epoch\": ep_idx, \"state_dict\": state, \"samples\": small_samples})\n",
    "\n",
    "    # Optionally capture an initial snapshot\n",
    "    if checkpoint_interval and checkpoint_interval > 0:\n",
    "        _capture_snapshot(0)\n",
    "\n",
    "    # Now we do the training loop for # epochs defined in `epochs`\n",
    "    for ep in range(epochs):\n",
    "        d_losses=[]\n",
    "        g_losses=[]\n",
    "        real_scores=[]\n",
    "        fake_scores=[]\n",
    "        for (xb,) in loader:\n",
    "            # Send the data to the GPU, if using.\n",
    "            xb = xb.to(device)\n",
    "\n",
    "            # Take a Discriminator step\n",
    "            z = torch.randn(xb.size(0), noise_dim, device=device)\n",
    "            with torch.no_grad():\n",
    "                x_fake = G(z)\n",
    "            opt_d.zero_grad()\n",
    "            d_real = D(xb)\n",
    "            d_fake = D(x_fake)\n",
    "            loss_d = bce(d_real, torch.ones_like(d_real)) + bce(d_fake, torch.zeros_like(d_fake))\n",
    "            loss_d.backward()\n",
    "            opt_d.step()\n",
    "            d_losses.append(float(loss_d.detach().cpu().item()))\n",
    "            real_scores.append(d_real.mean().item())\n",
    "            fake_scores.append(d_fake.mean().item())\n",
    "\n",
    "            # Take a Generator step\n",
    "            z = torch.randn(xb.size(0), noise_dim, device=device)\n",
    "            opt_g.zero_grad()\n",
    "            xg = G(z)\n",
    "            dg = D(xg)\n",
    "            loss_g = bce(dg, torch.ones_like(dg))\n",
    "            loss_g.backward()\n",
    "            opt_g.step()\n",
    "            g_losses.append(float(loss_g.detach().cpu().item()))\n",
    "        \n",
    "        # We'll record some epoch metrics\n",
    "        with torch.no_grad():\n",
    "            z_eval = torch.randn(2048, noise_dim, device=device)\n",
    "            samples = G(z_eval)\n",
    "            div = compute_diversity_metric(samples)\n",
    "\n",
    "        # Now we'll record the metrics for plotting later and reporting\n",
    "        hist.d_loss.append(float(np.mean(d_losses)))\n",
    "        hist.g_loss.append(float(np.mean(g_losses)))\n",
    "        hist.diversity.append(div)\n",
    "        hist.real_scores.append(float(np.mean(real_scores)))\n",
    "        hist.fake_scores.append(float(np.mean(fake_scores)))\n",
    "\n",
    "        # Optionally capture a checkpoint snapshot\n",
    "        if checkpoint_interval and checkpoint_interval > 0 and ((ep + 1) % checkpoint_interval == 0):\n",
    "            _capture_snapshot(ep + 1)\n",
    "\n",
    "        if (ep+1) % max(1, print_every) == 0 or ep==0:\n",
    "            print(f\"Epoch {ep+1:03d}/{epochs} | D {hist.d_loss[-1]:.3f} | G {hist.g_loss[-1]:.3f} | Div {div:.3f}\")\n",
    "    return G, D, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "G, D, H = train_vanilla_gan(\n",
    "    X_ring, epochs=120, batch_size=256,\n",
    "    lr_g=1e-5,\n",
    "    lr_d=1e-3,\n",
    "    hidden_dim=256, noise_dim=2, \n",
    "    checkpoint_interval=10,\n",
    "    print_every=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diagnostics(H, X_ring, G, noise_dim=2, title_suffix=' (GAN w/ D >> G)')\n",
    "plot_latent_interpolation(G, noise_dim=2, title_suffix=' (GAN w/ D >> G)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "We can also take a look at this interactively, by viewing various snapshots during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive checkpoint browser (requires ipywidgets)\n",
    "if interact is None:\n",
    "    print(\"ipywidgets not available. Install ipywidgets to use the interactive checkpoint browser.\")\n",
    "else:\n",
    "    from ipywidgets import IntSlider, Output, VBox, Label\n",
    "\n",
    "    out = Output()\n",
    "\n",
    "    def show_checkpoint(idx: int):\n",
    "        if not hasattr(H, \"snapshots\") or len(H.snapshots) == 0:\n",
    "            with out:\n",
    "                print(\"No checkpoints available. Rerun training with checkpoint_interval>0 to populate snapshots.\")\n",
    "            return\n",
    "        cp = H.snapshots[idx]\n",
    "        # Load generator state into a temporary generator instance\n",
    "        G_temp = build_generator(noise_dim=2, hidden_dim=256)\n",
    "        # load state dict (ensure tensors are moved to device)\n",
    "        state = {k: v.to(device) for k, v in cp[\"state_dict\"].items()}\n",
    "        G_temp.load_state_dict(state)\n",
    "\n",
    "        with out:\n",
    "            out.clear_output(wait=True)\n",
    "            # Plot diagnostics and latent interpolation for this checkpoint\n",
    "            print(f\"Checkpoint: index={idx} | epoch={cp['epoch']}\")\n",
    "            plot_model_diagnostics(H, X_ring, G_temp, noise_dim=2, title_suffix=f\" (Checkpoint ep {cp['epoch']})\")\n",
    "            plot_latent_interpolation(G_temp, noise_dim=2, title_suffix=f\" (Checkpoint ep {cp['epoch']})\", real_samples=X_ring, latent_limits=ring_latent_limits, data_limits=ring_data_limits)\n",
    "\n",
    "    def make_slider():\n",
    "        if not hasattr(H, \"snapshots\") or len(H.snapshots) == 0:\n",
    "            print(\"No checkpoints available. Rerun training with checkpoint_interval>0 to populate snapshots.\")\n",
    "            return\n",
    "        max_idx = len(H.snapshots) - 1\n",
    "        slider = IntSlider(value=0, min=0, max=max_idx, step=1, description='Checkpoint')\n",
    "        def on_change(change):\n",
    "            if change['name'] == 'value':\n",
    "                show_checkpoint(change['new'])\n",
    "        slider.observe(on_change)\n",
    "        display(VBox([Label('Checkpoint browser'), slider, out]))\n",
    "        # show initial\n",
    "        show_checkpoint(0)\n",
    "\n",
    "    make_slider()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Experiment 2: When the Generator Overpowers the Discriminator\n",
    "\n",
    "Above we saw what happens when the Discriminator overpowers the Generator. But what about the other way around? The next experiment will help illuminate what can go wrong.\n",
    "\n",
    "::: {.callout-tip appearance=\"default\"}\n",
    "### Experiment: What happens when the Generator overpowers the Discriminator?\n",
    "Below you can change the relative learning rates of the Discriminator versus the Generator to give the Generator a significant advantage:\n",
    "\n",
    "- What happens when we turn the tables on the Discriminator, and give the Generator comparatively more power?\n",
    "- How are the loss plots this time significantly different than what you saw in the first experiment?\n",
    "- As you look through the training curves as well as snapshots during training, what do you notice about the generated samples? Why would an underpowered Discriminator lead to this behavior? (Hint: Play out conceptually in your head how the generator would respond in cases where the Discriminator is very weak or slow.)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "G, D, H = train_vanilla_gan(\n",
    "    X_ring, epochs=120, batch_size=256,\n",
    "    lr_g=1e-4,\n",
    "    lr_d=1e-5,\n",
    "    hidden_dim=256, noise_dim=2, \n",
    "    checkpoint_interval=10,\n",
    "    print_every=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diagnostics(H, X_ring, G, noise_dim=2, title_suffix=' (GAN w/ G >> D)')\n",
    "plot_latent_interpolation(G, noise_dim=2, title_suffix=' (GAN w/ G >> D)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "\n",
    "You can also view this interactively at different training snapshots below, which might help illuminate some behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive checkpoint browser (requires ipywidgets)\n",
    "if interact is None:\n",
    "    print(\"ipywidgets not available. Install ipywidgets to use the interactive checkpoint browser.\")\n",
    "else:\n",
    "    from ipywidgets import IntSlider, Output, VBox, Label\n",
    "\n",
    "    out = Output()\n",
    "\n",
    "    def show_checkpoint(idx: int):\n",
    "        if not hasattr(H, \"snapshots\") or len(H.snapshots) == 0:\n",
    "            with out:\n",
    "                print(\"No checkpoints available. Rerun training with checkpoint_interval>0 to populate snapshots.\")\n",
    "            return\n",
    "        cp = H.snapshots[idx]\n",
    "        # Load generator state into a temporary generator instance\n",
    "        G_temp = build_generator(noise_dim=2, hidden_dim=256)\n",
    "        # load state dict (ensure tensors are moved to device)\n",
    "        state = {k: v.to(device) for k, v in cp[\"state_dict\"].items()}\n",
    "        G_temp.load_state_dict(state)\n",
    "\n",
    "        with out:\n",
    "            out.clear_output(wait=True)\n",
    "            # Plot diagnostics and latent interpolation for this checkpoint\n",
    "            print(f\"Checkpoint: index={idx} | epoch={cp['epoch']}\")\n",
    "            plot_model_diagnostics(H, X_ring, G_temp, noise_dim=2, title_suffix=f\" (Checkpoint ep {cp['epoch']})\")\n",
    "            plot_latent_interpolation(G_temp, noise_dim=2, title_suffix=f\" (Checkpoint ep {cp['epoch']})\", real_samples=X_ring, latent_limits=ring_latent_limits, data_limits=ring_data_limits)\n",
    "\n",
    "    def make_slider():\n",
    "        if not hasattr(H, \"snapshots\") or len(H.snapshots) == 0:\n",
    "            print(\"No checkpoints available. Rerun training with checkpoint_interval>0 to populate snapshots.\")\n",
    "            return\n",
    "        max_idx = len(H.snapshots) - 1\n",
    "        slider = IntSlider(value=0, min=0, max=max_idx, step=1, description='Checkpoint')\n",
    "        def on_change(change):\n",
    "            if change['name'] == 'value':\n",
    "                show_checkpoint(change['new'])\n",
    "        slider.observe(on_change)\n",
    "        display(VBox([Label('Checkpoint browser'), slider, out]))\n",
    "        # show initial\n",
    "        show_checkpoint(0)\n",
    "\n",
    "    make_slider()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Experiment 3: Effect of Discriminator Spectral Normalization\n",
    "\n",
    "Part of the issue in Experiment 1 (where the Discriminator overpowers the Generator) is that the Discriminator can make very sudden and extreme changes to its decision boundary, which the Generator cannot keep up with. One way to mitigate this is to use Spectral Normalization on the Discriminator weights, which constrains the Lipschitz constant of the Discriminator function, and thus prevents it from making extreme changes to its decision boundary.\n",
    "\n",
    "::: {.callout-tip appearance=\"default\"}\n",
    "### Experiment: How does Spectral Normalization affect the Discriminator?\n",
    "Below we will activate `spectral_normalization=True` in the Discriminator, and see how this affects the training dynamics when the Discriminator is given a significant learning rate advantage over the Generator. \n",
    "\n",
    "- Keeping the learning rates from Experiment 1, what do you now notice about the Discriminator's training losses?\n",
    "- Do different learning rates modulate this effect? You can repeat a version of Experiment 2 here if you are curious.\n",
    "- In what ways has applying this spectral normalization improved the model performance? In what ways has it made it worse? Why do you think this is? (Hint: Think about what Spectral Normalization controls within a network, and what effect that would have if applied to the Discriminator in a mini-max game against the Generator.)\n",
    "- Looking only at the various training curves, do you notice the effect of the Spectral normalization? Where do the effect of the Spectral Normalization show itself within the diagnostics?\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "G, D, H = train_vanilla_gan(\n",
    "    X_ring, epochs=240, batch_size=256,\n",
    "    lr_g=1e-4,\n",
    "    lr_d=5e-4,\n",
    "    spectral_normalization=True,\n",
    "    hidden_dim=256, noise_dim=2, \n",
    "    print_every=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diagnostics(H, X_ring, G, noise_dim=2, title_suffix=' (GAN w/ D_spec_norm >> G)')\n",
    "plot_latent_interpolation(G, noise_dim=2, title_suffix=' (GAN w/ D_spec_norm >> G)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "We have seen above how the mini-max formulation of a Generative Adversarial Network leads to unwanted training dynamics that need to be carefully tuned if the GAN is to perform well. In large part, we can see that this is the result of the two networks fighting each other. But is this fight really necessary? After all, the Generator is trying to learn the data distribution, and the Discriminator is just a tool to help it do so, because we didn't have a clear way to map the generated samples to the real samples in a way that directly allowed us to directly optimize something like an MSE. \n",
    "\n",
    "In the next notebook, we will see how we can reformulate this problem in terms of Optimal Transport, which will allow us to sidestep some of these issues and remove some of the problems cased by the mini-max game."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4me-student",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
