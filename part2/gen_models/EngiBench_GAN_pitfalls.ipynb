{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: GAN Training Pitfalls on Engineering Problems\n",
    "author: EngiBench Team\n",
    "date: 'October 15 2025'\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "        code-summary: \"Show Code\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Training Pitfalls on Engineering Design Problems\n",
    "\n",
    "This notebook demonstrates different training configurations for Generative Adversarial Networks (GANs) on real engineering design problems from EngiBench. We'll use the `beams2d` problem to explore how varying the learning rate balance between generator and discriminator affects training dynamics and output quality.\n",
    "\n",
    "We use the **cgan_2d** architecture (fully-connected layers) which is simpler and faster than CNN-based models.\n",
    "\n",
    "## Experiments\n",
    "\n",
    "We'll run three training configurations with different learning rate ratios:\n",
    "1. **Discriminator Overpowers Generator**: Very low generator LR, very high discriminator LR\n",
    "2. **Generator Overpowers Discriminator**: Very high generator LR, very low discriminator LR  \n",
    "3. **Balanced Training**: Standard balanced learning rate configuration\n",
    "\n",
    "Compare the training metrics and generated designs to understand the effects of each configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "If you get a `ValueError` about tensor size mismatch:\n",
    "\n",
    "```\n",
    "ValueError: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1]))\n",
    "```\n",
    "\n",
    "**This means the kernel has an old cached version of the functions. Follow these steps:**\n",
    "\n",
    "1. **Restart Kernel**: Go to `Kernel → Restart Kernel` in the menu\n",
    "2. **Clear Output** (optional): Go to `Cell → All Output → Clear`  \n",
    "3. **Run All Above**: Click on the first experiment cell, then `Cell → Run All Above`\n",
    "4. **Run Validation Cell**: There's a validation cell right before the experiments - run it to confirm\n",
    "5. **Run Your Experiment**: Now you can run the experiment cells\n",
    "\n",
    "The notebook has been fixed - the issue is just that Jupyter caches function definitions in memory until you restart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important: Setup Instructions\n",
    "\n",
    "**Before running experiments, you MUST run all cells in order:**\n",
    "\n",
    "1. Run the \"Setup and Imports\" cell below\n",
    "2. Run the \"Load EngiBench Problem\" cell\n",
    "3. Run the \"Model Architectures\" cell  \n",
    "4. Run the \"Training History and Utilities\" cell ← **Essential!**\n",
    "\n",
    "If you get shape mismatch errors, restart the kernel and re-run all cells above before running experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "from typing import NamedTuple\n",
    "\n",
    "from engibench.utils.all_problems import BUILTIN_PROBLEMS\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Device selection\n",
    "if th.backends.mps.is_available():\n",
    "    device = th.device(\"mps\")\n",
    "elif th.cuda.is_available():\n",
    "    device = th.device(\"cuda\")\n",
    "else:\n",
    "    device = th.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load EngiBench Problem\n",
    "\n",
    "We'll use the `beams2d` problem - a 2D structural design optimization problem where we need to generate optimal beam structures for different loading conditions and volume fraction constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load problem\n",
    "problem_id = \"beams2d\"\n",
    "problem = BUILTIN_PROBLEMS[problem_id]()\n",
    "problem.reset(seed=42)\n",
    "\n",
    "design_shape = problem.design_space.shape\n",
    "conditions = problem.conditions_keys\n",
    "n_conds = len(conditions)\n",
    "\n",
    "print(f\"Problem: {problem_id}\")\n",
    "print(f\"Design shape: {design_shape}\")\n",
    "print(f\"Conditions: {[c[0] for c in conditions]}\")\n",
    "print(f\"Number of conditions: {n_conds}\")\n",
    "\n",
    "# Load dataset\n",
    "training_ds = problem.dataset.with_format(\"torch\", device=device)[\"train\"]\n",
    "print(f\"\\nTraining samples: {len(training_ds)}\")\n",
    "\n",
    "# Show a few example designs\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "axes = axes.flatten()\n",
    "for i in range(8):\n",
    "    design = training_ds[\"optimal_design\"][i].cpu().numpy().reshape(design_shape)\n",
    "    axes[i].imshow(design, cmap='gray')\n",
    "    axes[i].set_title(f'Sample {i+1}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle('Example Training Designs from EngiBench', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architectures\n",
    "\n",
    "We'll use conditional GAN architectures from `cgan_2d` - simpler fully-connected networks that are faster to train and make failure modes more obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Conditional GAN generator using fully-connected layers with better conditioning.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim: int, n_conds: int, design_shape: tuple):\n",
    "        super().__init__()\n",
    "        self.design_shape = design_shape\n",
    "\n",
    "        def block(in_feat: int, out_feat: int, normalize: bool = True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        # Separate paths for noise and conditions (like cGAN best practices)\n",
    "        # This gives conditions more \"weight\" in the network\n",
    "        self.noise_path = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),\n",
    "        )\n",
    "        \n",
    "        self.cond_path = nn.Sequential(\n",
    "            nn.Linear(n_conds, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Main generation path (after concatenation)\n",
    "        self.model = nn.Sequential(\n",
    "            *block(256, 512),  # 128 + 128 = 256\n",
    "            *block(512, 1024),\n",
    "            *block(1024, 2048),\n",
    "            nn.Linear(2048, int(np.prod(design_shape))),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z: th.Tensor, conds: th.Tensor) -> th.Tensor:\n",
    "        \"\"\"Generate design from noise and conditions.\n",
    "        \n",
    "        Args:\n",
    "            z: Latent noise (B, latent_dim)\n",
    "            conds: Conditions (B, n_conds)\n",
    "            \n",
    "        Returns:\n",
    "            Generated design (B, *design_shape)\n",
    "        \"\"\"\n",
    "        # Process noise and conditions separately\n",
    "        z_feat = self.noise_path(z)\n",
    "        c_feat = self.cond_path(conds)\n",
    "        \n",
    "        # Concatenate and generate\n",
    "        gen_input = th.cat((z_feat, c_feat), -1)\n",
    "        design = self.model(gen_input)\n",
    "        return design.view(design.size(0), *self.design_shape)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Conditional GAN discriminator using fully-connected layers with better conditioning.\n",
    "    \n",
    "    Args:\n",
    "        design_shape: Shape of input designs\n",
    "        n_conds: Number of conditional features\n",
    "        spectral_norm: If True, applies spectral normalization to constrain Lipschitz constant\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, design_shape: tuple, n_conds: int, spectral_norm: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Helper to optionally apply spectral normalization\n",
    "        def maybe_spectral_norm(layer):\n",
    "            if spectral_norm:\n",
    "                return nn.utils.spectral_norm(layer)\n",
    "            return layer\n",
    "        \n",
    "        # Separate paths for design and conditions\n",
    "        self.design_path = nn.Sequential(\n",
    "            maybe_spectral_norm(nn.Linear(int(np.prod(design_shape)), 512)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.cond_path = nn.Sequential(\n",
    "            nn.Linear(n_conds, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Main discrimination path (after concatenation)\n",
    "        self.model = nn.Sequential(\n",
    "            maybe_spectral_norm(nn.Linear(640, 512)),  # 512 + 128 = 640\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            maybe_spectral_norm(nn.Linear(512, 256)),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            maybe_spectral_norm(nn.Linear(256, 1)),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, design: th.Tensor, conds: th.Tensor) -> th.Tensor:\n",
    "        \"\"\"Classify design as real or fake given conditions.\n",
    "        \n",
    "        Args:\n",
    "            design: Input design (B, *design_shape)\n",
    "            conds: Conditions (B, n_conds)\n",
    "            \n",
    "        Returns:\n",
    "            Validity score (B, 1)\n",
    "        \"\"\"\n",
    "        design_flat = design.view(design.size(0), -1)\n",
    "        \n",
    "        # Process design and conditions separately\n",
    "        d_feat = self.design_path(design_flat)\n",
    "        c_feat = self.cond_path(conds)\n",
    "        \n",
    "        # Concatenate and discriminate\n",
    "        d_in = th.cat((d_feat, c_feat), -1)\n",
    "        return self.model(d_in)\n",
    "\n",
    "print(\"Model architectures defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training History and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingHistory(NamedTuple):\n",
    "    \"\"\"Store training metrics.\"\"\"\n",
    "    d_loss: list[float]\n",
    "    g_loss: list[float]\n",
    "    d_real_score: list[float]\n",
    "    d_fake_score: list[float]\n",
    "    diversity: list[float]\n",
    "    snapshots: list[dict]\n",
    "\n",
    "\n",
    "def compute_diversity(samples: th.Tensor) -> float:\n",
    "    \"\"\"Compute diversity of generated samples using pairwise distances.\"\"\"\n",
    "    samples_flat = samples.reshape(samples.size(0), -1)\n",
    "    dists = th.cdist(samples_flat, samples_flat, p=2)\n",
    "    n = samples.size(0)\n",
    "    if n <= 1:\n",
    "        return 0.0\n",
    "    return dists.sum().item() / (n * (n - 1))\n",
    "\n",
    "\n",
    "def train_cgan(problem_id: str, n_epochs: int, batch_size: int, latent_dim: int,\n",
    "               lr_gen: float, lr_disc: float, seed: int = 42,\n",
    "               spectral_norm: bool = False,\n",
    "               snapshot_interval: int = 10, print_every: int = 20) -> tuple:\n",
    "    \"\"\"Train conditional GAN on EngiBench problem.\n",
    "    \n",
    "    Args:\n",
    "        spectral_norm: Apply spectral normalization to discriminator (Lipschitz-1 constraint)\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    th.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    th.backends.cudnn.deterministic = True\n",
    "    \n",
    "    # Load problem\n",
    "    problem = BUILTIN_PROBLEMS[problem_id]()\n",
    "    problem.reset(seed=seed)\n",
    "    design_shape = problem.design_space.shape\n",
    "    n_conds = len(problem.conditions_keys)\n",
    "    \n",
    "    # Prepare data\n",
    "    training_ds = problem.dataset.with_format(\"torch\", device=device)[\"train\"]\n",
    "    dataset = TensorDataset(\n",
    "        training_ds[\"optimal_design\"][:].flatten(1),\n",
    "        *[training_ds[key][:] for key in problem.conditions_keys]\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize models\n",
    "    generator = Generator(latent_dim, n_conds, design_shape).to(device)\n",
    "    discriminator = Discriminator(design_shape, n_conds, spectral_norm=spectral_norm).to(device)\n",
    "    \n",
    "    # Loss and optimizers\n",
    "    criterion = nn.BCELoss()\n",
    "    opt_gen = optim.Adam(generator.parameters(), lr=lr_gen, betas=(0.5, 0.999))\n",
    "    opt_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Training history\n",
    "    history = TrainingHistory([], [], [], [], [], [])\n",
    "    \n",
    "    # Helper to save snapshot\n",
    "    def save_snapshot(epoch: int):\n",
    "        with th.no_grad():\n",
    "            z = th.randn(64, latent_dim, device=device)\n",
    "            conds_list = [training_ds[key][:] for key in problem.conditions_keys]\n",
    "            conds = th.stack([c[th.randint(0, len(c), (64,))] for c in conds_list], dim=1)\n",
    "            samples = generator(z, conds).cpu()\n",
    "        \n",
    "        state = {k: v.cpu().clone() for k, v in generator.state_dict().items()}\n",
    "        history.snapshots.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"state_dict\": state,\n",
    "            \"samples\": samples[:16],\n",
    "            \"conditions\": conds[:16].cpu()\n",
    "        })\n",
    "    \n",
    "    if snapshot_interval > 0:\n",
    "        save_snapshot(0)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        d_losses, g_losses, d_real_scores, d_fake_scores = [], [], [], []\n",
    "        \n",
    "        for data in dataloader:\n",
    "            designs = data[0]\n",
    "            conds = th.stack(data[1:], dim=1)\n",
    "            \n",
    "            batch_size_actual = designs.size(0)\n",
    "            valid = th.ones(batch_size_actual, device=device)\n",
    "            fake = th.zeros(batch_size_actual, device=device)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            opt_disc.zero_grad()\n",
    "            z = th.randn(batch_size_actual, latent_dim, device=device)\n",
    "            gen_designs = generator(z, conds)\n",
    "            \n",
    "            real_pred = discriminator(designs.view(batch_size_actual, *design_shape), conds).squeeze()\n",
    "            fake_pred = discriminator(gen_designs.detach(), conds).squeeze()\n",
    "            \n",
    "            d_loss = (criterion(real_pred, valid) + criterion(fake_pred, fake)) / 2\n",
    "            d_loss.backward()\n",
    "            opt_disc.step()\n",
    "            \n",
    "            # Train Generator\n",
    "            opt_gen.zero_grad()\n",
    "            z = th.randn(batch_size_actual, latent_dim, device=device)\n",
    "            gen_designs = generator(z, conds)\n",
    "            gen_pred = discriminator(gen_designs, conds).squeeze()\n",
    "            \n",
    "            g_loss = criterion(gen_pred, valid)\n",
    "            g_loss.backward()\n",
    "            opt_gen.step()\n",
    "            \n",
    "            # Record metrics\n",
    "            d_losses.append(d_loss.item())\n",
    "            g_losses.append(g_loss.item())\n",
    "            d_real_scores.append(real_pred.mean().item())\n",
    "            d_fake_scores.append(fake_pred.mean().item())\n",
    "        \n",
    "        # Compute diversity\n",
    "        with th.no_grad():\n",
    "            z_eval = th.randn(100, latent_dim, device=device)\n",
    "            conds_eval = conds[:100] if len(conds) >= 100 else conds\n",
    "            samples_eval = generator(z_eval[:len(conds_eval)], conds_eval)\n",
    "            diversity = compute_diversity(samples_eval)\n",
    "        \n",
    "        # Epoch metrics\n",
    "        history.d_loss.append(np.mean(d_losses))\n",
    "        history.g_loss.append(np.mean(g_losses))\n",
    "        history.d_real_score.append(np.mean(d_real_scores))\n",
    "        history.d_fake_score.append(np.mean(d_fake_scores))\n",
    "        history.diversity.append(diversity)\n",
    "        \n",
    "        if snapshot_interval > 0 and (epoch + 1) % snapshot_interval == 0:\n",
    "            save_snapshot(epoch + 1)\n",
    "        \n",
    "        if (epoch + 1) % print_every == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1:03d}/{n_epochs} | D loss: {history.d_loss[-1]:.4f} | \"\n",
    "                  f\"G loss: {history.g_loss[-1]:.4f} | D(real): {history.d_real_score[-1]:.3f} | \"\n",
    "                  f\"D(fake): {history.d_fake_score[-1]:.3f} | Diversity: {history.diversity[-1]:.2f}\")\n",
    "    \n",
    "    return generator, discriminator, history\n",
    "\n",
    "\n",
    "def plot_training_diagnostics(history: TrainingHistory, title_suffix: str = \"\"):\n",
    "    \"\"\"Plot training diagnostics - 2x2 grid.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    epochs = np.arange(1, len(history.d_loss) + 1)\n",
    "    \n",
    "    # Training Losses\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(epochs, history.d_loss, label='Discriminator', linewidth=2.5, alpha=0.8, color='C0')\n",
    "    ax.plot(epochs, history.g_loss, label='Generator', linewidth=2.5, alpha=0.8, color='C1')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Loss', fontsize=12)\n",
    "    ax.set_title('Training Losses', fontsize=13, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Discriminator Predictions\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(epochs, history.d_real_score, label='D(real)', linewidth=2.5, alpha=0.8, color='C2')\n",
    "    ax.plot(epochs, history.d_fake_score, label='D(fake)', linewidth=2.5, alpha=0.8, color='C3')\n",
    "    ax.axhline(y=0.5, color='k', linestyle='--', alpha=0.4, linewidth=1.5, label='Random (0.5)')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Discriminator Predictions', fontsize=13, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    \n",
    "    # Loss Ratio\n",
    "    ax = axes[1, 0]\n",
    "    loss_ratio = np.array(history.d_loss) / (np.array(history.g_loss) + 1e-8)\n",
    "    ax.plot(epochs, loss_ratio, linewidth=2.5, alpha=0.8, color='purple')\n",
    "    ax.axhline(y=1.0, color='k', linestyle='--', alpha=0.4, linewidth=1.5, label='Balanced (1.0)')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('D_loss / G_loss', fontsize=12)\n",
    "    ax.set_title('Loss Ratio', fontsize=13, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Diversity\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(epochs, history.diversity, linewidth=2.5, alpha=0.8, color='teal')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Avg Pairwise Distance', fontsize=12)\n",
    "    ax.set_title('Sample Diversity', fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'Training Diagnostics{title_suffix}', fontsize=15, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_generated_vs_real_designs(generator, latent_dim: int, n_conds: int, \n",
    "                                   design_shape: tuple[int, int], title_suffix: str = \"\",\n",
    "                                   n_samples: int = 8):\n",
    "    \"\"\"Visualize generated designs next to real designs with same conditions.\"\"\"\n",
    "    generator.eval()\n",
    "    \n",
    "    # Load problem and dataset\n",
    "    problem = BUILTIN_PROBLEMS[problem_id]()\n",
    "    training_ds = problem.dataset.with_format(\"torch\", device=device)[\"train\"]\n",
    "    conds_list = [training_ds[key][:] for key in problem.conditions_keys]\n",
    "    \n",
    "    # Create grid of diverse conditions\n",
    "    n_per_dim = int(np.sqrt(n_samples))\n",
    "    if n_conds == 2:\n",
    "        c1_vals = th.linspace(conds_list[0].min(), conds_list[0].max(), n_per_dim, device=device)\n",
    "        c2_vals = th.linspace(conds_list[1].min(), conds_list[1].max(), n_per_dim, device=device)\n",
    "        c1_grid, c2_grid = th.meshgrid(c1_vals, c2_vals, indexing='ij')\n",
    "        conds = th.stack([c1_grid.flatten(), c2_grid.flatten()], dim=1)[:n_samples]\n",
    "    else:\n",
    "        indices = th.linspace(0, len(conds_list[0]) - 1, n_samples, device=device).long()\n",
    "        conds = th.stack([c[indices] for c in conds_list], dim=1)\n",
    "    \n",
    "    # Generate designs\n",
    "    with th.no_grad():\n",
    "        z = th.randn(len(conds), latent_dim, device=device)\n",
    "        gen_designs = generator(z, conds).cpu().numpy()\n",
    "    \n",
    "    # Find closest real designs for same conditions\n",
    "    # Stack all training conditions into a matrix (N_train, n_conds)\n",
    "    all_train_conds = th.stack(conds_list, dim=1)\n",
    "    \n",
    "    real_designs = []\n",
    "    for i in range(len(conds)):\n",
    "        # Find training sample with closest conditions\n",
    "        cond_dists = th.cdist(conds[i:i+1], all_train_conds)\n",
    "        closest_idx = cond_dists.argmin().item()  # Convert 0-d tensor to Python int\n",
    "        real_design = training_ds[\"optimal_design\"][closest_idx].cpu().numpy().reshape(design_shape)\n",
    "        real_designs.append(real_design)\n",
    "    \n",
    "    conds_np = conds.cpu().numpy()\n",
    "    \n",
    "    # Plot: each row shows [Real | Generated] for same conditions\n",
    "    fig, axes = plt.subplots(n_samples, 2, figsize=(8, n_samples * 2))\n",
    "    if n_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Real design\n",
    "        axes[i, 0].imshow(real_designs[i], cmap='viridis', vmin=-1, vmax=1)\n",
    "        axes[i, 0].axis('off')\n",
    "        if i == 0:\n",
    "            axes[i, 0].set_title('Real Design', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Generated design\n",
    "        axes[i, 1].imshow(gen_designs[i], cmap='viridis', vmin=-1, vmax=1)\n",
    "        axes[i, 1].axis('off')\n",
    "        if i == 0:\n",
    "            axes[i, 1].set_title('Generated Design', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Condition label on the left\n",
    "        cond_str = \", \".join([f\"{conds_np[i, j]:.2f}\" for j in range(n_conds)])\n",
    "        fig.text(0.02, 1 - (i + 0.5) / n_samples, f\"C: [{cond_str}]\", \n",
    "                va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle(f'Real vs Generated Designs{title_suffix}', fontsize=14, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout(rect=[0.1, 0, 1, 0.99])\n",
    "    plt.show()\n",
    "\n",
    "print(\"Training utilities defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Discriminator Overpowers Generator\n",
    "\n",
    "**Training Configuration:**\n",
    "- `lr_gen = 1e-5` (very low)\n",
    "- `lr_disc = 1e-3` (very high) \n",
    "- Discriminator learns 100x faster than generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT 1: Discriminator Overpowers Generator\")\n",
    "print(\"Settings: lr_gen=1e-5, lr_disc=1e-3 (D is 100x faster)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "latent_dim = 100\n",
    "gen_exp1, disc_exp1, hist_exp1 = train_cgan(\n",
    "    problem_id=\"beams2d\",\n",
    "    n_epochs=100,\n",
    "    batch_size=32,\n",
    "    latent_dim=latent_dim,\n",
    "    lr_gen=1e-5,\n",
    "    lr_disc=1e-3,\n",
    "    seed=42,\n",
    "    snapshot_interval=10,\n",
    "    print_every=20,\n",
    "    spectral_norm=False\n",
    ")\n",
    "\n",
    "plot_training_diagnostics(hist_exp1, \" - Exp 1\")\n",
    "plot_generated_vs_real_designs(gen_exp1, latent_dim, n_conds, design_shape, \" - Exp 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Generator Overpowers Discriminator\n",
    "\n",
    "**Training Configuration:**\n",
    "- `lr_gen = 5e-4` (very high)\n",
    "- `lr_disc = 1e-5` (very low)\n",
    "- Generator learns 50x faster than discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT 2: Generator Overpowers Discriminator\")\n",
    "print(\"Settings: lr_gen=5e-4, lr_disc=1e-5 (G is 50x faster)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "latent_dim = 100\n",
    "\n",
    "gen_exp2, disc_exp2, hist_exp2 = train_cgan(\n",
    "    problem_id=\"beams2d\",\n",
    "    n_epochs=100,\n",
    "    batch_size=32,\n",
    "    latent_dim=latent_dim,\n",
    "    lr_gen=5e-4,\n",
    "    lr_disc=1e-5,\n",
    "    seed=42,\n",
    "    snapshot_interval=10,\n",
    "    print_every=20,\n",
    "    spectral_norm=False\n",
    ")\n",
    "\n",
    "plot_training_diagnostics(hist_exp2, \" - Exp 2\")\n",
    "plot_generated_vs_real_designs(gen_exp2, latent_dim, n_conds, design_shape, \" - Exp 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Balanced Training\n",
    "\n",
    "**Training Configuration:**\n",
    "- `lr_gen = 1e-4` (standard)\n",
    "- `lr_disc = 4e-4` (standard, 4x ratio)\n",
    "- Typical balanced learning rate configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT 3: Balanced Training\")\n",
    "print(\"Settings: lr_gen=1e-4, lr_disc=4e-4 (standard 1:4 ratio)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "latent_dim = 100\n",
    "\n",
    "gen_exp3, disc_exp3, hist_exp3 = train_cgan(\n",
    "    problem_id=\"beams2d\",\n",
    "    n_epochs=100,\n",
    "    batch_size=32,\n",
    "    latent_dim=latent_dim,\n",
    "    lr_gen=1e-4,\n",
    "    lr_disc=4e-4,\n",
    "    seed=42,\n",
    "    snapshot_interval=10,\n",
    "    print_every=20,\n",
    "    spectral_norm=False\n",
    ")\n",
    "\n",
    "plot_training_diagnostics(hist_exp3, \" - Exp 3\")\n",
    "plot_generated_vs_real_designs(gen_exp3, latent_dim, n_conds, design_shape, \" - Exp 3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
