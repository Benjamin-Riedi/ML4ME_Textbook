{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "---\n",
    "title: Optimal Transport for Generative Models\n",
    "author: Mark Fuge\n",
    "date: 'October 12 2025'\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "        code-summary: \"Show Code\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In the previous notebooks, we explored how Generative Adversarial Networks (GANs) use a minimax game between a Generator and Discriminator to learn data distributions. However, we saw that this adversarial training can lead to instability, mode collapse, and other training difficulties. In this notebook, we'll explore an alternative approach based on **Optimal Transport (OT)**, which provides a more direct way to measure the distance between probability distributions.\n",
    "\n",
    "The key insight is this: instead of training a discriminator to distinguish real from fake samples, we can directly minimize the distance between the generated distribution and the real data distribution using optimal transport metrics. This often leads to more stable training and better coverage of the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Optional interactive widgets\n",
    "try:\n",
    "    from ipywidgets import interact, FloatSlider, IntSlider\n",
    "    widgets_available = True\n",
    "except Exception:\n",
    "    interact = None\n",
    "    FloatSlider = IntSlider = None\n",
    "    widgets_available = False\n",
    "\n",
    "# Import geomloss for optimal transport\n",
    "try:\n",
    "    from geomloss import SamplesLoss\n",
    "    geomloss_available = True\n",
    "except ImportError:\n",
    "    print(\"Warning: geomloss not available. Install with: pip install geomloss\")\n",
    "    geomloss_available = False\n",
    "\n",
    "# Import shared utilities from the local module\n",
    "from gen_models_utilities import (\n",
    "    device, create_ring_gaussians,\n",
    "    make_loader, compute_diversity_metric, plot_model_diagnostics, plot_latent_interpolation\n",
    ")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-muted')\n",
    "sns.set_context('talk')\n",
    "\n",
    "#print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## What is Optimal Transport?\n",
    "\n",
    "Let's go back to our earlier analogy where we were imagining probability distributions as two piles of sand, and our goal is to transform one pile into the other. Optimal transport addresses the question: **What is the most efficient way to move the sand from one configuration to another?**\n",
    "\n",
    "More formally, given two probability distributions $z$ and $x$, optimal transport finds a transport plan $\\pi$ that moves mass from $z$ to $x$ while minimizing the total *transport cost*. There are many possible definitions of cost here, and it is common to think of a distance as a form of cost, with the **Wasserstein distance** (also called the Earth Mover's Distance) as a common one with the technical form:\n",
    "$$\n",
    "W_p(z, x) = \\left( \\inf_{\\pi \\in \\Pi(z, x)} \\int \\|x - y\\|^p \\, d\\pi(x, y) \\right)^{1/p}\n",
    "$$\n",
    "\n",
    "where $\\Pi(z, x)$ is the set of all joint distributions with marginals $z$ and $x$.\n",
    "\n",
    "OK, so far so good in principle -- I just need to find the transport plan that minimizes some p-norm over all joint distributions of $z$ and $x$. However, in practice, this is not so straightforward, since finding a minimum over all possible joint distributions of $z$ and $x$ is not so computationally tractable.[^1]\n",
    "\n",
    "[^1]: At a high level, this is due to a combinatoric assignment problem in something called the coupling matrix, where you are trying to match generated and real datapoints to one another and optimize for the lowest distance. Because this assignment matrix is ultimately a binary matrix, this makes it not easily differentiable.\n",
    "\n",
    "Instead, of computing the Wasserstein distance directly, we will compute an approximate version of it that regularizes the transport plan, and is called computing the **Sinkhorn Divergence**. Going over the specific implementation details of the Sinkhorn Divergence (which rely on Sinkhorn Iteration and knowledge of doubly stochastic matrices) are beyond the scope of what I want to cover in these notebooks, but interested students can check out [Computational Optimal Transport](https://arxiv.org/abs/1803.00567) by Gabriel Peyr√© and Marco Cuturi for further details. \n",
    "\n",
    "The important thing to know in the context of a course at this level is that the Sinkhorn Divergence can only *approximate* the true Wasserstein distance, and that it does so via what is often called a \"blur\" parameter. This parameter is essentially a smoothing term that determines how much we penalize the complexity of the transport map. Some small amount of blur will help us compute gradients and use the Sinkhorn Divergence in ML model training, but too much of this will prevent a model from capturing fine details in the data distribution. You will see an interactive example of this next before we move on to using OT for GAN training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Simple Optimal Transport Example \n",
    "\n",
    "Let's start with a familiar and concrete example that we have been using in the prior GAN notebooks. We'll take a simple 2D Gaussian and compute its optimal transport to our ring of Gaussians dataset. We'll visualize the transport map by computing the transport vectors (i.e., in what direction we move the probability mass) and also demonstrate how moving in those directions shifts our 2D Gaussian towards the ring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ring dataset\n",
    "X_ring, y_ring = create_ring_gaussians(n_samples=2000)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "sc = ax.scatter(X_ring[:, 0], X_ring[:, 1], c=y_ring, cmap='tab10', s=15, alpha=0.6)\n",
    "plt.colorbar(sc, label='Mode index')\n",
    "ax.set_title('Target: Ring of Gaussians')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.axis('equal')\n",
    "ax.grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Now what we will do below is place a simple 2D Gaussian distribution centered at the origin, and then compute the Sinkhorn Divergence (loss) between each point in the real dataset and each point in the simple 2D Gaussian. Using Automatic Differentiation, we can then compute how to move each point in the generated distribution to minimize this loss. We will plot a sample of these gradient vectors so that you can see what the transport map looks like and also take a (very large) step in that direction for each point, so you can see the visual effect of the transport.\n",
    "\n",
    "In this case, we are only taking a single, giant step along the transport map for pedagogical purposes, but in reality (and as we will do later), you would move slowly along the transport map over many iterations to gradually morph the generated distribution into the real data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "# Create a simple 2D Gaussian source distribution\n",
    "n_source = 2000\n",
    "source_samples = np.random.randn(n_source, 2).astype(np.float32) * 0.5\n",
    "\n",
    "# Convert to torch tensors\n",
    "source_torch = torch.from_numpy(source_samples).to(device)\n",
    "target_torch = torch.from_numpy(X_ring).to(device)\n",
    "\n",
    "# Compute optimal transport using Sinkhorn algorithm\n",
    "# The blur parameter controls entropic regularization (larger = more regularization)\n",
    "sinkhorn_loss = SamplesLoss(\"sinkhorn\", p=2, blur=0.01, scaling=0.9)\n",
    "\n",
    "# Compute transport plan by taking gradient\n",
    "source_torch.requires_grad_(True)\n",
    "loss = sinkhorn_loss(source_torch, target_torch)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "# This is effectively a giant step size so the we can visualize the gradients\n",
    "# and see a meaningful change in the distribution.\n",
    "# In reality, we would take much smaller steps than this and do it over iterations\n",
    "magnitude_scaling = 2000\n",
    "\n",
    "# The gradient points in the direction of optimal transport\n",
    "transport_direction = source_torch.grad.detach().cpu().numpy()\n",
    "transported_points = source_samples + transport_direction*magnitude_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the transport\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10,16))\n",
    "\n",
    "# Source distribution\n",
    "axes[0].scatter(source_samples[:, 0], source_samples[:, 1], \n",
    "                s=15, alpha=0.5, c='tab:blue')\n",
    "axes[0].scatter(X_ring[:, 0], X_ring[:, 1], \n",
    "                s=8, alpha=0.2, c='lightgray', label='Target')\n",
    "axes[0].set_title('Source: 2D Gaussian')\n",
    "axes[0].set_xlabel('$x_1$')\n",
    "axes[0].set_ylabel('$x_2$')\n",
    "axes[0].axis('equal')\n",
    "axes[0].grid(True, alpha=0.2)\n",
    "\n",
    "# Transport vectors\n",
    "# Sample a subset for visualization clarity\n",
    "idx_subset = np.random.choice(n_source, size=25, replace=False)\n",
    "axes[1].scatter(X_ring[:, 0], X_ring[:, 1], \n",
    "                s=8, alpha=0.2, c='lightgray', label='Target')\n",
    "axes[1].quiver(source_samples[idx_subset, 0], source_samples[idx_subset, 1],\n",
    "                magnitude_scaling/1.5*transport_direction[idx_subset, 0], \n",
    "                magnitude_scaling/1.5*transport_direction[idx_subset, 1],\n",
    "                angles='xy', scale_units='xy', scale=0.5, width=0.005, \n",
    "                color='tab:orange', alpha=0.5)\n",
    "axes[1].set_title('Optimal Transport Vectors')\n",
    "axes[1].set_xlabel('$x_1$')\n",
    "axes[1].set_ylabel('$x_2$')\n",
    "axes[1].axis('equal')\n",
    "axes[1].grid(True, alpha=0.2)\n",
    "axes[1].legend()\n",
    "\n",
    "# Transported distribution\n",
    "axes[2].scatter(X_ring[:, 0], X_ring[:, 1], \n",
    "                s=8, alpha=0.2, c='lightgray', label='Target')\n",
    "axes[2].scatter(transported_points[:, 0], transported_points[:, 1], \n",
    "                s=15, alpha=0.5, c='tab:red', label='Transported')\n",
    "axes[2].set_title('After Transport')\n",
    "axes[2].set_xlabel('$x_1$')\n",
    "axes[2].set_ylabel('$x_2$')\n",
    "axes[2].axis('equal')\n",
    "axes[2].grid(True, alpha=0.2)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sinkhorn divergence: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "In the visualization above, you can see how optimal transport naturally moves mass from the source Gaussian to the target ring distribution. The transport vectors (middle plot) show the direction and magnitude of how each point should move to minimize the total transport cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "\n",
    "::: {.callout-tip appearance=\"default\"}\n",
    "### Experiment: Effect of Sinkhorn Divergence Parameters\n",
    "The Sinkhorn divergence has several key parameters that affect the transport:\n",
    "\n",
    "1. **`blur` ($\\epsilon$)**: Controls the amount of entropic regularization. Larger values make the transport \"smoother\" but less accurate.\n",
    "2. **`p`**: The p-norm used for measuring distances. This is typically `p=1` for Manhattan distance (sum of absolute differences) or `p=2` for Euclidean distance (standard L2 norm)).\n",
    "\n",
    "Use the slider below to gain intuition about changing the effects of these three parameters:\n",
    "- What effect does moving from a small blur to a large blur have?\n",
    "- How does the transport pattern differ between p=1 and p=2?\n",
    "\n",
    "\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if geomloss_available and widgets_available:\n",
    "    def explore_pnorm(p: int = 2, blur: float = 0.05):\n",
    "        # Create source distribution\n",
    "        source_samples = np.random.randn(1000, 2).astype(np.float32) * 0.5\n",
    "        source_torch = torch.from_numpy(source_samples).to(device)\n",
    "        target_torch = torch.from_numpy(X_ring[:1000]).to(device)\n",
    "        \n",
    "        # Compute transport\n",
    "        sinkhorn_loss = SamplesLoss(\"sinkhorn\", p=p, blur=blur, scaling=0.9)\n",
    "        source_torch.requires_grad_(True)\n",
    "        loss = sinkhorn_loss(source_torch, target_torch)\n",
    "        loss.backward()\n",
    "        \n",
    "        magnitude_scaling = 1500\n",
    "        \n",
    "        transport_direction = source_torch.grad.detach().cpu().numpy()\n",
    "        transported_points = source_samples + transport_direction * magnitude_scaling\n",
    "        \n",
    "        # Visualize\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Transport vectors\n",
    "        idx_subset = np.random.choice(1000, size=150, replace=False)\n",
    "        axes[0].scatter(X_ring[:, 0], X_ring[:, 1], \n",
    "                       s=8, alpha=0.15, c='lightgray', label='Target')\n",
    "        axes[0].quiver(source_samples[idx_subset, 0], source_samples[idx_subset, 1],\n",
    "                      magnitude_scaling/1.5*transport_direction[idx_subset, 0], \n",
    "                      magnitude_scaling/1.5*transport_direction[idx_subset, 1],\n",
    "                      angles='xy', scale_units='xy', scale=1, width=0.003, \n",
    "                      color='tab:orange', alpha=0.7)\n",
    "        axes[0].set_title(f'Transport Vectors (p={p}, blur={blur:.3f})')\n",
    "        axes[0].set_xlabel('$x_1$')\n",
    "        axes[0].set_ylabel('$x_2$')\n",
    "        axes[0].axis('equal')\n",
    "        axes[0].grid(True, alpha=0.2)\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Result\n",
    "        axes[1].scatter(X_ring[:, 0], X_ring[:, 1], \n",
    "                       s=8, alpha=0.2, c='lightgray', label='Target')\n",
    "        axes[1].scatter(transported_points[:, 0], transported_points[:, 1], \n",
    "                       s=15, alpha=0.5, c='tab:red', label='Transported')\n",
    "        axes[1].set_title(f'After Transport (loss={loss.item():.4f})')\n",
    "        axes[1].set_xlabel('$x_1$')\n",
    "        axes[1].set_ylabel('$x_2$')\n",
    "        axes[1].axis('equal')\n",
    "        axes[1].grid(True, alpha=0.2)\n",
    "        axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    interact(explore_pnorm, \n",
    "             p=IntSlider(min=1, max=2, step=1, value=2, description='p-norm'),\n",
    "             blur=FloatSlider(min=0.01, max=0.7, step=0.01, value=0.05, description='Blur (Œµ)'))\n",
    "elif not geomloss_available:\n",
    "    print(\"GeomLoss not available. Please install it to run this experiment.\")\n",
    "else:\n",
    "    print(\"ipywidgets not available. Please install it for interactive controls.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Building an Entropic OT GAN\n",
    "\n",
    "Now that we are beginning to understand optimal transport and how it might be useful, let's try to use it to train a generative model. Instead of using a discriminator (as in standard GANs), we'll directly minimize the Sinkhorn divergence between generated samples and real data. We can do this because the Sinkhorn divergence is now substituting for the original role of the Discriminator (i.e., to move the generator closer to real-world data), and so the Discriminator is no longer necessary.[^2]\n",
    "\n",
    "[^2]: One could argue that we are no longer really doing a \"GAN\" here because we do not have an \"Adversary\" now that the Discriminator is gone, so it is perhaps misleading to call it this, but since earlier papers refer to this style of Generative Model as an Entropic GAN, I will be consistent with them, even if it isn't the best name in my view.\n",
    "\n",
    "This approach has several advantages:\n",
    "\n",
    "1. **No discriminator needed**: We only need the network for the Generator now, which is simpler and we fewer total parameters to train. We don't need to worry about two learning rates or different capacities in each network, like we saw in the GAN Pitfalls notebook.\n",
    "2. **More stable to train**: Since we no longer have a minimax game to balance, we do not have to contend with oscillatory behavior of the optimizer and potentially getting trapped in a loop. This can also equate to faster training times as a result, if the learning rate is suitably tuned.\n",
    "3. **Better coverage**: OT naturally encourages covering all modes of the data distribution, rather than hoping that a Discriminator pushes the Generator to cover all modes.\n",
    "4. **Meaningful gradients**: Because OT computes a pairwise distance among all data points, so long as the gradient of our distance/cost measure remains finite and non-zero at far away distances, we can still perform useful gradient descent steps in the generative model. This is not the case for some other cost functions (such as the KL Divergence) where gradients might vanish if the two distributions are not close enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "class GeneratorMLP(nn.Module):\n",
    "    \"\"\"Simple MLP generator for 2D data.\"\"\"\n",
    "    def __init__(self, noise_dim: int = 2, hidden_dim: int = 256, out_dim: int = 2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(noise_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class SinkhornHistory:\n",
    "    \"\"\"Training history for Sinkhorn-based generator.\"\"\"\n",
    "    loss: List[float]\n",
    "    diversity: List[float]\n",
    "\n",
    "def train_sinkhorn_generator(\n",
    "    data: np.ndarray,\n",
    "    noise_dim: int = 2,\n",
    "    batch_size: int = 256,\n",
    "    epochs: int = 200,\n",
    "    lr: float = 1e-3,\n",
    "    hidden_dim: int = 256,\n",
    "    blur: float = 0.05,\n",
    "    p: int = 2,\n",
    "    print_every: int = 50\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train a generator using Sinkhorn divergence.\n",
    "    \n",
    "    Args:\n",
    "        data: Real data samples (numpy array)\n",
    "        noise_dim: Dimension of latent noise\n",
    "        batch_size: Batch size for training\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        hidden_dim: Hidden dimension for generator\n",
    "        blur: Entropic regularization parameter\n",
    "        p: p-norm for distance metric\n",
    "        print_every: Print progress every N epochs\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (trained generator, training history)\n",
    "    \"\"\"\n",
    "    if not geomloss_available:\n",
    "        raise ImportError(\"GeomLoss is required for Sinkhorn training. Install with: pip install geomloss\")\n",
    "    \n",
    "    # Setup data loader\n",
    "    loader = make_loader(data, batch_size)\n",
    "    \n",
    "    # Initialize generator and optimizer\n",
    "    G = GeneratorMLP(noise_dim=noise_dim, hidden_dim=hidden_dim, out_dim=2).to(device)\n",
    "    optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Setup Sinkhorn loss\n",
    "    sinkhorn_loss = SamplesLoss(\"sinkhorn\", p=p, blur=blur, scaling=0.9, debias=True)\n",
    "    \n",
    "    # Training history\n",
    "    history = SinkhornHistory(loss=[], diversity=[])\n",
    "    \n",
    "    print(f\"Training Sinkhorn OT Generator for {epochs} epochs...\")\n",
    "    print(f\"Parameters: blur={blur}, p={p}, lr={lr}, hidden_dim={hidden_dim}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for (real_batch,) in loader:\n",
    "            real_batch = real_batch.to(device)\n",
    "            \n",
    "            # Generate fake samples\n",
    "            z = torch.randn(batch_size, noise_dim, device=device)\n",
    "            fake_batch = G(z)\n",
    "            \n",
    "            # Compute Sinkhorn divergence\n",
    "            loss = sinkhorn_loss(fake_batch, real_batch)\n",
    "            \n",
    "            # Update generator\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses.append(float(loss.detach().cpu().item()))\n",
    "        \n",
    "        # Record metrics\n",
    "        mean_loss = float(np.mean(epoch_losses))\n",
    "        \n",
    "        # Compute diversity\n",
    "        with torch.no_grad():\n",
    "            z_eval = torch.randn(2048, noise_dim, device=device)\n",
    "            samples = G(z_eval)\n",
    "            diversity = compute_diversity_metric(samples)\n",
    "        \n",
    "        history.loss.append(mean_loss)\n",
    "        history.diversity.append(diversity)\n",
    "        \n",
    "        if (epoch + 1) % print_every == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1:03d}/{epochs} | Loss: {mean_loss:.4f} | Diversity: {diversity:.4f}\")\n",
    "    return G, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Let's train the OT generator on our ring dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if geomloss_available:\n",
    "    # Train the generator\n",
    "    G_ot, H_ot = train_sinkhorn_generator(\n",
    "        X_ring,\n",
    "        epochs=200,\n",
    "        batch_size=256,\n",
    "        noise_dim=2,\n",
    "        hidden_dim=256,\n",
    "        blur=0.02,\n",
    "        p=2,\n",
    "        lr=1e-3,\n",
    "        print_every=50\n",
    "    )\n",
    "    \n",
    "    # Generate and visualize samples\n",
    "    with torch.no_grad():\n",
    "        z_test = torch.randn(2000, 2, device=device)\n",
    "        generated_samples = G_ot(z_test).cpu().numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "    ax.scatter(X_ring[:, 0], X_ring[:, 1], \n",
    "              s=10, alpha=0.2, c='lightgray', label='Real')\n",
    "    ax.scatter(generated_samples[:, 0], generated_samples[:, 1], \n",
    "              s=12, alpha=0.5, c='tab:orange', label='Generated (OT)')\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.set_title('Entropic OT GAN: Real vs Generated Samples')\n",
    "    ax.legend()\n",
    "    ax.axis('equal')\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"GeomLoss not available. Please install it to train the OT generator.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Let's use our standard diagnostic plots from the GAN notebooks to evaluate the OT generator's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diagnostics(\n",
    "    H_ot, \n",
    "    X_ring, \n",
    "    G_ot, \n",
    "    noise_dim=2, \n",
    "    title_suffix=' (Sinkhorn OT)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_interpolation(\n",
    "    G_ot, \n",
    "    noise_dim=2, \n",
    "    title_suffix=' (Sinkhorn OT)',\n",
    "    real_samples=X_ring\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this notebook, we explored how Optimal Transport provides an alternative to adversarial training for generative models:\n",
    "\n",
    "1. **Optimal Transport Intuition**: OT finds the most efficient way to transform one distribution into another, providing meaningful gradients even when distributions don't overlap.\n",
    "\n",
    "2. **Sinkhorn Divergence**: By adding entropic regularization, we make OT computationally tractable while preserving most benefits. The `blur` parameter controls the trade-off between precision and smoothness.\n",
    "\n",
    "3. **Entropic OT GANs**: We can train generative models by directly minimizing Sinkhorn divergence, eliminating the need for a discriminator and the associated minimax game.\n",
    "\n",
    "4. **Advantages over Traditional GANs**:\n",
    "   - Simpler architecture (no discriminator)\n",
    "   - More stable training (no adversarial dynamics)\n",
    "   - Better mode coverage (OT naturally spreads mass)\n",
    "   - Meaningful loss values (directly related to distribution distance)\n",
    "\n",
    "5. **Trade-offs**: OT-based training requires careful tuning of the `blur` parameter and can be computationally expensive for very large datasets. In particular, OT-based distances are very good when the source and target distributions are already well aligned, but less so when the transport distance is high (since then the optimal map computed by Sinkhorn Iteration may not be as discriminative). As a consequence, many real-world applications of OT in a generative model context use OT as a fine-tuning step after doing an alignment or registration step (e.g., when matching a mesh of an organ to data from a CT scan).\n",
    "\n",
    "So, at this point we have addressed one of the key weaknesses of GANs -- the instability of adversarial training -- by removing the discriminator entirely and replacing it with a direct measure of distribution distance. However, there is still one big missing piece to be desired by Entropic GAN-style models. While we have a better forward map now of $f(z) \\rightarrow x$ via the generator, we still do not have an inverse map $f^{-1}(x) \\rightarrow z$ that would allow us to encode real data points back into the latent space. This is something that Variational Autoencoders (VAEs) and Normalizing Flows provide, and is where we turn our attention next. We will see that the general concept of OT will raise its head again after Normalizing Flows in the context of something called \"Flow Matching\", and we will return to this in a few notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4me-student",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
