{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBj4Ak8Y1Je8"
   },
   "source": [
    "## PS1 Part 2: Unsupervised Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-N00-Zh1JfC"
   },
   "source": [
    "### Toy Dataset\n",
    "For this problem, you will use the data file hb.csv. The input is 2,280 data points, each of which is 7\n",
    "dimensional (i.e., input csv is 2280 rows by 7 columns). Use Principal Component Analysis\n",
    "(either an existing library, or through your own implementation by taking the SVD of the\n",
    "Covariance Matrix) for the follow tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8E8A7Yc1JfD"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas\n",
    "url = \"https://raw.githubusercontent.com/IDEALLab/ML4ME_Textbook/main/problems/hb.csv\"\n",
    "data = pandas.read_csv(url,header=None)\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSqT200D1JfE"
   },
   "source": [
    "### Task 1\n",
    "Assuming that the 7-dimensional space is excessive, you would like to reduce the dimension of the space. However, what dimensionality of space should we reduce it to? To determine this we need to compute its intrinsic dimensionality. Plot the relative value of the information content of each of the principal components and compare them.\n",
    "\n",
    "Note: this information content is called the “explained variance” of each component, but you can also get this from the magnitude of the singular values. This plot is sometimes called a “Scree Plot”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2uSzsro1JfF"
   },
   "outputs": [],
   "source": [
    "# Code Here\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(data)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.pie(pca.explained_variance_ratio_,labels=[f\"{pca.explained_variance_ratio_[i]*100:.4f}%\" for i in range(7)])\n",
    "plt.title(\"Principal Component relative values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wKtGmLl1JfF"
   },
   "source": [
    "**Question:** Approximately how many components dominate the space?, and what does this tell us about the intrinsic dimensionality of the space?\n",
    "\n",
    "**Response**: 2 components describe the space completely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukv-5inf1JfG"
   },
   "source": [
    "#### Task 2\n",
    "Now use PCA to project the 7-dimensional points on the K-dimensional space (where K is your answer from above) and plot the points. (For K=1,2, or 3, use a 1, 2, or 3D plot, respectively. For 4+ dimensions, use a grid of pairwise 2D Plots, like the Scatter Matrix we used in class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-D_aZuyO1JfG"
   },
   "outputs": [],
   "source": [
    "# Code Here\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data)\n",
    "print(data_pca)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(data_pca[:,0],data_pca[:,1],marker='|')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hsa2R2pv1JfH"
   },
   "source": [
    "**Question:** What do you notice?\n",
    "\n",
    "**Response**: seems like the underlying dimension is formed like a skunk ^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-r2dUE5d1JfH"
   },
   "source": [
    "### Topology Optimization Dataset\n",
    "For this problem, you will be using unsupervised linear models to help understand and interpret the results of a mechanical optimization problem. Specifically, to understand the solution space generated by a topology optimization code; that is, the results of finding the optimal geometries for minimizing the compliance of various bridge structures with different loading conditions. The input consists of 1,000 images of optimized material distribution for a beam as described in *Figure 1*. A symmetrical boundary condition, left side, is used to reduce the analysis to only half. Also, a rolling support is included at the lower right corner. Notice that the rolling support is the only support in the vertical direction.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center>\n",
    "<img src=\"beam_description.jpg\" alt=\"Beam description\" style=\"max-width:100%;height:auto;\">\n",
    "</center>\n",
    "<center>Figure 1: Left: Nx-by-Ny design domain for topology optimization problem. Right: Example loading configuration and resulting optimal topology. Two external forces, Fi, were applied to the beam at random nodes represented by (xi, yi) coordinates.<sup>1</sup></center>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Use Principal Component Analysis (either an existing library, or through your own implementation by taking the SVD of the Covariance Matrix) for the follow tasks.\n",
    "\n",
    "<sup>1. This problems data is based on the problem setup seen in the following paper: Ulu, E., Zhang, R., & Kara, L. B. (2016). A data-driven investigation and estimation of optimal topologies under variable loading configurations. *Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization*, 4(2), 61-72.</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-x580lhq1JfH",
    "outputId": "4139b9f7-52fc-4387-c562-869b08341205"
   },
   "outputs": [],
   "source": [
    "# To help you get started, the below code will load the images from the associated image folder:\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "#im_dir = './topo_opt_runs/'\n",
    "url = \"https://raw.githubusercontent.com/IDEALLab/ML4ME_Textbook/main/problems/topo_opt_runs.zip\"\n",
    "resp = requests.get(url)\n",
    "resp.raise_for_status()\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "images = []\n",
    "for name in sorted(zf.namelist()):\n",
    "    with zf.open(name) as f:\n",
    "        img = Image.open(f).convert('L')\n",
    "        images.append(np.asarray(img))\n",
    "\n",
    "height,width = images[0].shape\n",
    "print('The images are {:d} pixels high and {:d} pixels wide'.format(height,width))\n",
    "\n",
    "# Print matrix corresponding to the image:\n",
    "print(images[-1])\n",
    "# And show example image, so you can see how matrix correponds:\n",
    "\n",
    "images = np.array(images)\n",
    "images = images.reshape(1000,-1)\n",
    "#check this again\n",
    "\n",
    "print(images.shape)\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYO4WApZ1JfI"
   },
   "source": [
    "### Task 1: Scree/Singular Value Plot\n",
    "As with the toy example, assume that the 94,178-dimensional space is excessive. You would like to reduce the dimension of the image space. First compute its intrinsic dimensionality. For this application, \"good enough\" means capturing 95% of the variance in the dataset. How many dimensions are needed to capture at least 95% of the variance in the provided dataset? Store your answer in numDimsNeeded. (Hint: A Scree plot may be helpful, though visual inspection of a graph may not be precise enough.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qXlVKc-1JfJ"
   },
   "outputs": [],
   "source": [
    "n_components = 170\n",
    "pca = PCA(n_components=n_components)\n",
    "# pca = PCA()\n",
    "pca.fit(images)\n",
    "\n",
    "cumulative_explained_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "number_of_vars_to_95 = np.argmax(cumulative_explained_var_ratio>.95)\n",
    "print(pca.n_samples_)\n",
    "print(pca.n_features_in_)\n",
    "# print(cumulative_explained_var_ratio)\n",
    "# print(number_of_vars_to_95)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.pie(pca.explained_variance_ratio_)\n",
    "plt.title(\"Principal Component relative values\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(cumulative_explained_var_ratio,label=\"Cumulative Explained Variance\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.xlabel(\"Principal Components\")\n",
    "# plt.xticks(np.arange(n_components))\n",
    "plt.vlines(number_of_vars_to_95,\n",
    "        np.min(cumulative_explained_var_ratio),1.0,\n",
    "        colors=\"k\",linestyles='dashed',\n",
    "        label = f\"95% Explained Variance @ {number_of_vars_to_95}\")\n",
    "plt.legend()\n",
    "plt.title(\"PCA Cumulative Explained Variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYDuciqm1JfJ"
   },
   "source": [
    "**Question:** Approximately how many components dominate the space? What does this tell us about the intrinsic dimensionality of the space?\n",
    "\n",
    "**Response**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVOVQpFu1JfJ"
   },
   "source": [
    "168 components explain 95% of the variance. Intrinsic dimensionality is therefore 168 (depending on where you set your threshhold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLcYLT551JfJ"
   },
   "source": [
    "### Task 2: Principal Components\n",
    "Now plot the first 5 principal components. Hint: looking at each of these top 5 principal components; do they make sense physically, in terms of what it means for where material in the bridge is placed? Compare, for example, the differences between the 1st and 2nd principal component?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXCmByx31JfJ"
   },
   "outputs": [],
   "source": [
    "# comp_imgs = components_.reshape((components_.shape[0], 217, 434))\n",
    "\n",
    "for n_components in range(2,5):\n",
    "    estimator = PCA(n_components=n_components, whiten=False)\n",
    "    estimator.fit(images)  # fit PCA with fewer components to illustrate reconstructions\n",
    "    # pick an index to reconstruct (e.g., first image)\n",
    "    comp_imgs = estimator.components_.reshape((estimator.components_.shape[0], 217, 434))\n",
    "    idx = 0\n",
    "    x = images[idx]  # original flattened image (shape: n_features)\n",
    "    # project to low-dim: z = (x - mean) @ components_.T\n",
    "    z = (x - estimator.mean_) @ estimator.components_.T  # shape (n_components,)\n",
    "    # reconstruct from the low-dim code: x_hat = z @ components + mean\n",
    "    x_hat = z @ estimator.components_ + estimator.mean_  # shape (n_features,)\n",
    "\n",
    "    # reshape for visualization\n",
    "    img_orig = x.reshape(217, 434)\n",
    "    img_recon = x_hat.reshape(217, 434)\n",
    "    img_mean = estimator.mean_.reshape(217, 434)\n",
    "\n",
    "    # Now plot original, mean, and reconstructed images side-by-side\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(img_orig, cmap='gray')\n",
    "    plt.title('Original')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(img_mean, cmap='gray')\n",
    "    plt.title('Mean')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(img_recon, cmap='gray')\n",
    "    plt.title(f'Reconstructed (k={n_components})')\n",
    "    plt.axis('off')\n",
    "    plt.suptitle(f'PCA Reconstruction using {n_components} components')\n",
    "    plt.show()\n",
    "\n",
    "# visualize first 5 principal components as images (signed weights)\n",
    "num_show = 5\n",
    "plt.figure(figsize=(15,3))\n",
    "for i in range(num_show):\n",
    "    plt.subplot(1, num_show, i+1)\n",
    "    plt.imshow(pca.components_[i].reshape(217,434), cmap='RdBu_r')\n",
    "    plt.title(f'PC {i+1}')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('First 5 PCA components (as images)', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml4me-student",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
