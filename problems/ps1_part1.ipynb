{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHsB5YzJxOMN"
   },
   "source": [
    "## PS1 Part 1: Linear Models and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7UittANxOMQ"
   },
   "source": [
    "### Preamble\n",
    "We'll be loading some CO2 concentration data that is a commonly used dataset for model building of time series prediction. You will build a few baseline linear models and assess them using some of the tools we discussed in class. Which model is best? Let's find out.\n",
    "\n",
    "First let's just load the data and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLvRqWiPxOMQ",
    "outputId": "01fb91e3-d0b6-4572-b3e4-2fa1bc662fa1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "sns.set_context('notebook')\n",
    "\n",
    "# Fetch the data\n",
    "mauna_lao = fetch_openml('mauna-loa-atmospheric-co2', as_frame = False)\n",
    "print(mauna_lao.DESCR)\n",
    "data = mauna_lao.data\n",
    "# Assemble the day/time from the data columns so we can plot it\n",
    "d1958 = datetime(year=1958,month=1,day=1)\n",
    "time = [datetime(int(d[0]),int(d[1]),int(d[2])) for d in data]\n",
    "X = np.array([1958+(t-d1958)/timedelta(days=365.2425) for t in time]).T\n",
    "X = X.reshape(-1,1)  # Make it a column to make scikit happy\n",
    "y = np.array(mauna_lao.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j52MD0b9xOMR",
    "outputId": "e748e355-746a-436a-a870-efeab3ee37fa"
   },
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.figure(figsize=(10,5))    # Initialize empty figure\n",
    "plt.scatter(X, y, c='k',s=1) # Scatterplot of data\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(r\"CO$_2$ in ppm\")\n",
    "plt.title(r\"Atmospheric CO$_2$ concentration at Mauna Loa\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXIQYrIHxOMR",
    "outputId": "a3115c21-560a-4aa1-9581-b601fd694946"
   },
   "outputs": [],
   "source": [
    "y[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUC01N-1xOMR"
   },
   "source": [
    "### Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLiWXNI8xOMS"
   },
   "source": [
    "Construct the following linear models:\n",
    "1. Model 1: \"Vanilla\" Linear Regression, that is, where $CO_2 = a+b \\cdot time$\n",
    "2. Model 2: Quadratic Regression, where $CO_2 = a+b \\cdot t + c\\cdot t^2$\n",
    "3. Model 3: A more complex \"linear\" model with the following additive terms $CO_2=a+b\\cdot t+c\\cdot sin(\\omega\\cdot t)$:\n",
    "  * a linear (in time) term\n",
    "  * a sinusoidal additive term with period such that the peak-to-peak of the sinsusoid is roughly ~1 year and phase shift of zero (set $\\omega$ as appropriate to match the peaks)\n",
    "4. Model 4: A \"linear\" model with the following additive terms ($CO_2=a+b\\cdot t+c\\cdot t^2+d\\cdot sin(\\omega\\cdot t)$:\n",
    "  * a quadratic (in time) polynomial\n",
    "  * a sinusoidal additive term with period such that the peak-to-peak of the sinsusoid is roughly ~1 year and phase shift of zero (set $\\omega$ as appropriate to match the peaks)\n",
    "  \n",
    "Evauate these models using **the appropriate kind of Cross Validation** for each of the following amounts of Training data:\n",
    "1. N=50 Training Data Points\n",
    "2. N=100\n",
    "3. N=200\n",
    "4. N=500\n",
    "5. N=1000\n",
    "6. N=2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e94Lo4zxOMS"
   },
   "source": [
    "**Question**: Before you even construct the models or do any coding below, what is your initial guess or intuition behind how each of those four models will perform? Note: there is no right or wrong answer to this part of the assignment and this question will only be graded on completeness, not accuracy. It's intent is to get you to think about and write down your preliminary intuition regarding what you think will happen before you actually implement anything, based on your approximate understanding of how functions of the above complexity *should* perform as N increases.\n",
    "\n",
    "**Student Response:** \n",
    "Model 1 should average out the spikes and approximate the slope of overall increase of CO2\n",
    "Model 2 will perform similarly to Model 1 with the difference that it won't form a straight line, but have a slight curve\n",
    "Both of these Models should perform fairly well, even with few datapoints\n",
    "Model 3 should additionally incorporate the peaks. Of course with fewer datapoints it will struggle to find the correct amplitude.\n",
    "Model 4 should perform similarly to Model 2 but again have a slight curve representing the overall (growing) increase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJFlaewExOMS"
   },
   "source": [
    "**Question**: What is the appropriate kind of Cross Validation to perform in this case if we want a correct Out of Sample estimate of our Test MSE?\n",
    "\n",
    "**Student Response:** Time Series Split Cross Validation is appropriate, because to predict (potentially future) CO2 values we need to train our model to only train on past data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAAtymgExOMS"
   },
   "source": [
    "Now, for each of the above models and training data sizes:\n",
    "* Plot the predicted CO2 as a function of time, including the actual data, for each of the N=X training data examples. This should correspond to six plots (one for each amount of training data) if you plot all models on the same plot, or 6x4 = 24 plots if you plot each model and training data plot separately.\n",
    "* Create a [Learning Curve](https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html) plot for the model which plots its Training and Test MSE as a function of training data. That is, plot how Training and Testing MSE change as you increase the training data for each model. This could be a single plot for all four models (8 lines on the plot) or four different plots corresponding to the learning curve of each model separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2OTOXRZxOMS",
    "outputId": "18527223-3502-4389-af9a-3187d756bc39"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train_100 = X[:100]\n",
    "y_train_100 = y[:100]\n",
    "X_test = X[100:200]\n",
    "print(\"Shape of X_train_100: %s\" % str(X_train_100.shape))\n",
    "print(\"Beginning of X_train_100: %s\" % str(X_train_100[0:5]))\n",
    "print(\"Shape of y_train_100: %s\" % str(y_train_100.shape))\n",
    "print(\"Beginning of y_train_100: %s\" % str(y_train_100[0:5]))\n",
    "\n",
    "print('Shape of X_test: %s' % str(X_test.shape))\n",
    "print(\"Beginning of X_test: %s\" % str(X_test[0:5]))\n",
    "\n",
    "### Modify the below code. You can leave the code above as is. ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWxIZchZxOMT"
   },
   "outputs": [],
   "source": [
    "# Insert Modeling Building or Plotting code here\n",
    "# Note, you may implement these however you see fit\n",
    "# Ex: using an existing library, solving the Normal Eqns\n",
    "#     implementing your own SGD solver for them. Your Choice.\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class SinFeature(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, omega):\n",
    "        self.omega = omega\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.sin(self.omega * X).reshape(-1,1)\n",
    "\n",
    "omega = 2 * np.pi / 1.0  # yearly seasonality\n",
    "n_splits = 5\n",
    "\n",
    "sgd = SGDRegressor()\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "pipe1 = Pipeline([('poly', PolynomialFeatures(degree=1,\n",
    "                                             include_bias=False)),\n",
    "                 ('lr',LinearRegression())])\n",
    "pipe2 = Pipeline([('poly', PolynomialFeatures(degree=2,include_bias=False)),\n",
    "                  ('lr',LinearRegression())])\n",
    "pipe3 = Pipeline([('features', FeatureUnion([\n",
    "                        ('poly', PolynomialFeatures(degree=1,include_bias=False)),\n",
    "                        ('sin', SinFeature(omega=omega))])),\n",
    "                  ('lr',LinearRegression())])\n",
    "pipe4 = Pipeline([('features', FeatureUnion([\n",
    "                        ('poly', PolynomialFeatures(degree=2,include_bias=False)),\n",
    "                        ('sin', SinFeature(omega=omega))])),\n",
    "                  ('lr',LinearRegression())])\n",
    "\n",
    "N = [50,100,200,500,1000,2000]\n",
    "\n",
    "for n in N:\n",
    "    print(f\"N = {n}\")\n",
    "    random_indices = np.sort(np.random.choice(X.shape[0], n, replace=False))\n",
    "    randomY = y[random_indices]\n",
    "    randomX = X[random_indices]\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(randomX)):\n",
    "        pipe1.fit(randomX[train_index],randomY[train_index])\n",
    "        pipe2.fit(randomX[train_index],randomY[train_index])\n",
    "        pipe3.fit(randomX[train_index],randomY[train_index])\n",
    "        pipe4.fit(randomX[train_index],randomY[train_index])\n",
    "\n",
    "        # Visualize Time Split\n",
    "\n",
    "        pred = pipe4.predict(randomX[test_index])\n",
    "        pred1 = pipe4.predict(randomX[train_index])\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.scatter(X, y, c='k',s=0.5, alpha=0.5, label=\"Data\")\n",
    "        plt.scatter(randomX[train_index],randomY[train_index], label=\"train\",s=5)\n",
    "        plt.scatter(randomX[test_index],randomY[test_index], label=\"test\",s=5)\n",
    "        plt.scatter(randomX[train_index],pred1, label=\"train_pred\",s=5)\n",
    "        plt.scatter(randomX[test_index],pred, label=\"test_pred\",s=5)\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.title(f\"Time Split Nr {i+1}\")\n",
    "        plt.show()\n",
    "\n",
    "    pred1 = pipe1.predict(X)\n",
    "    pred2 = pipe2.predict(X)\n",
    "    pred3 = pipe3.predict(X)\n",
    "    pred4 = pipe4.predict(X)\n",
    "\n",
    "    # Calculate and print scores for each model\n",
    "    mse1 = mean_squared_error(y, pred1)\n",
    "    r2_1 = r2_score(y, pred1)\n",
    "    mse2 = mean_squared_error(y, pred2)\n",
    "    r2_2 = r2_score(y, pred2)\n",
    "    mse3 = mean_squared_error(y, pred3)\n",
    "    r2_3 = r2_score(y, pred3)\n",
    "    mse4 = mean_squared_error(y, pred4)\n",
    "    r2_4 = r2_score(y, pred4)\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.scatter(X, y, c='k',s=1, label=\"Data\")\n",
    "    plt.scatter(X,pred1, label=\"Model 1: MSE {:.2f}, R2 {:.3f}\".format(mse1, r2_1),s=5)\n",
    "    plt.scatter(X,pred2, label=\"Model 2: MSE {:.2f}, R2 {:.3f}\".format(mse2, r2_2),s=5)\n",
    "    plt.scatter(X,pred3, label=\"Model 3: MSE {:.2f}, R2 {:.3f}\".format(mse3, r2_3),s=5,alpha=0.5)\n",
    "    plt.scatter(X,pred4, label=\"Model 4: MSE {:.2f}, R2 {:.3f}\".format(mse4, r2_4),s=5,alpha=0.5)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(f\"N = {n}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66RBpEoixOMT"
   },
   "source": [
    "**Question**: Which Model appears to perform best in the N=50 or N=100 Condition? Why is this?\n",
    "\n",
    "**Student Response:** Although it goes against my intuition model 4 performs best everywhere. As seen in the plots it fairly accurately follows the curve and since we chose the \"correct\" $\\omega$ it only need to approximate the amplitude resulting in high accuracy.\n",
    "\n",
    "**Question**: Which Model appears to perform best as the N=200 to 500? Why is this?\n",
    "\n",
    "**Student Response:** Model 4\n",
    "\n",
    "**Question**: Which Model appears to perform best as N = 2000? Why is this?\n",
    "\n",
    "**Student Response:** Model 4"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml4me-student",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
